use anyhow::Result;
use lru::LruCache;
use once_cell::sync::Lazy;
use std::cell::RefCell;
use std::collections::{HashMap, HashSet};
use std::num::NonZeroUsize;
use std::path::Path;
use std::sync::atomic::{AtomicU64, Ordering};
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use tantivy::collector::TopDocs;
use tantivy::query::{AllQuery, BooleanQuery, Occur, Query, RangeQuery, RegexQuery, TermQuery};
use tantivy::schema::{IndexRecordOption, Term, Value};
use tantivy::snippet::SnippetGenerator;
use tantivy::{Index, IndexReader, Searcher, TantivyDocument};
use tokio::runtime::Handle;
use tokio::sync::mpsc;
use tokio::task::JoinHandle;

use rusqlite::Connection;

use crate::search::tantivy::fields_from_schema;

#[derive(Debug, Clone, Default, serde::Serialize)]
pub struct SearchFilters {
    pub agents: HashSet<String>,
    pub workspaces: HashSet<String>,
    pub created_from: Option<i64>,
    pub created_to: Option<i64>,
}

/// Indicates how a search result matched the query.
/// Used for ranking: exact matches rank higher than wildcard matches.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Default, serde::Serialize)]
#[serde(rename_all = "snake_case")]
pub enum MatchType {
    /// No wildcards - matched via exact term or edge n-gram prefix
    #[default]
    Exact,
    /// Matched via trailing wildcard (foo*)
    Prefix,
    /// Matched via leading wildcard (*foo) - uses regex
    Suffix,
    /// Matched via both wildcards (*foo*) - uses regex
    Substring,
    /// Matched via automatic wildcard fallback when exact search was sparse
    ImplicitWildcard,
}

impl MatchType {
    /// Returns a quality factor for ranking (1.0 = best, lower = less precise match)
    pub fn quality_factor(self) -> f32 {
        match self {
            MatchType::Exact => 1.0,
            MatchType::Prefix => 0.9,
            MatchType::Suffix => 0.8,
            MatchType::Substring => 0.7,
            MatchType::ImplicitWildcard => 0.6,
        }
    }
}

/// Type of suggestion for did-you-mean
#[derive(Debug, Clone, PartialEq, Eq, serde::Serialize)]
#[serde(rename_all = "snake_case")]
pub enum SuggestionKind {
    /// Typo correction (Levenshtein distance)
    SpellingFix,
    /// Try with wildcard prefix/suffix
    WildcardQuery,
    /// Remove restrictive filter
    RemoveFilter,
    /// Try different agent
    AlternateAgent,
    /// Broaden date range
    BroaderDateRange,
}

/// A "did-you-mean" suggestion when search returns zero hits.
#[derive(Debug, Clone, serde::Serialize)]
pub struct QuerySuggestion {
    /// What kind of suggestion this is
    pub kind: SuggestionKind,
    /// Human-readable description (e.g., "Did you mean: 'codex'?")
    pub message: String,
    /// The suggested query string (if query change)
    pub suggested_query: Option<String>,
    /// Suggested filters to apply (replaces current filters if Some)
    pub suggested_filters: Option<SearchFilters>,
    /// Shortcut key (1, 2, or 3) for quick apply in TUI
    pub shortcut: Option<u8>,
}

impl QuerySuggestion {
    fn spelling(_query: &str, corrected: &str) -> Self {
        Self {
            kind: SuggestionKind::SpellingFix,
            message: format!("Did you mean: \"{}\"?", corrected),
            suggested_query: Some(corrected.to_string()),
            suggested_filters: None,
            shortcut: None,
        }
    }

    fn wildcard(query: &str) -> Self {
        let wildcard_query = format!("*{}*", query.trim_matches('*'));
        Self {
            kind: SuggestionKind::WildcardQuery,
            message: format!("Try broader search: \"{}\"", wildcard_query),
            suggested_query: Some(wildcard_query),
            suggested_filters: None,
            shortcut: None,
        }
    }

    fn remove_agent_filter(current_agent: &str) -> Self {
        Self {
            kind: SuggestionKind::RemoveFilter,
            message: format!("Remove agent filter (currently: {})", current_agent),
            suggested_query: None,
            suggested_filters: Some(SearchFilters::default()),
            shortcut: None,
        }
    }

    fn try_agent(agent_slug: &str) -> Self {
        let mut filters = SearchFilters::default();
        filters.agents.insert(agent_slug.to_string());
        Self {
            kind: SuggestionKind::AlternateAgent,
            message: format!("Try searching in: {}", agent_slug),
            suggested_query: None,
            suggested_filters: Some(filters),
            shortcut: None,
        }
    }

    fn with_shortcut(mut self, key: u8) -> Self {
        self.shortcut = Some(key);
        self
    }
}

#[derive(Debug, Clone, serde::Serialize)]
pub struct SearchHit {
    pub title: String,
    pub snippet: String,
    pub content: String,
    pub score: f32,
    pub source_path: String,
    pub agent: String,
    pub workspace: String,
    pub created_at: Option<i64>,
    /// Line number in the source file where the matched message starts (1-indexed)
    pub line_number: Option<usize>,
    /// How this result matched the query (exact, prefix wildcard, etc.)
    #[serde(default)]
    pub match_type: MatchType,
}

/// Result of a search operation with metadata about how matches were found
#[derive(Debug, Clone)]
pub struct SearchResult {
    /// The search results
    pub hits: Vec<SearchHit>,
    /// Whether wildcard fallback was used (query had no/few exact matches)
    pub wildcard_fallback: bool,
    /// Cache metrics snapshot for observability/debug
    pub cache_stats: CacheStats,
    /// Did-you-mean suggestions when hits are empty or sparse
    pub suggestions: Vec<QuerySuggestion>,
}

pub struct SearchClient {
    reader: Option<(IndexReader, crate::search::tantivy::Fields)>,
    sqlite: Option<Connection>,
    prefix_cache: Mutex<CacheShards>,
    last_reload: Mutex<Option<Instant>>,
    last_generation: Mutex<Option<u64>>,
    reload_epoch: Arc<AtomicU64>,
    warm_tx: Option<mpsc::UnboundedSender<WarmJob>>,
    _warm_handle: Option<JoinHandle<()>>,
    // Shared for warm worker to read cache/filter logic; keep Arc to avoid clones of big data
    _shared_filters: Arc<Mutex<()>>, // placeholder lock to ensure Send/Sync; future warm prefill state
    metrics: Metrics,
    cache_namespace: String,
}

#[derive(Debug, Clone, Default, PartialEq, Eq)]
pub struct CacheStats {
    pub cache_hits: u64,
    pub cache_miss: u64,
    pub cache_shortfall: u64,
    pub reloads: u64,
    pub reload_ms_total: u128,
    pub total_cap: usize,
    pub total_cost: usize,
    /// Total evictions since client creation
    pub eviction_count: u64,
    /// Approximate bytes used by cache (rough estimate)
    pub approx_bytes: usize,
    /// Byte cap if set (0 = no byte limit)
    pub byte_cap: usize,
}

// Cache tuning: read from env to allow runtime override without recompiling.
// CASS_CACHE_SHARD_CAP controls per-shard entries; default 256.
static CACHE_SHARD_CAP: Lazy<usize> = Lazy::new(|| {
    std::env::var("CASS_CACHE_SHARD_CAP")
        .ok()
        .and_then(|v| v.parse::<usize>().ok())
        .filter(|v| *v > 0)
        .unwrap_or(256)
});

// Total cache cost across all shards; approximate “~2k entries” default.
static CACHE_TOTAL_CAP: Lazy<usize> = Lazy::new(|| {
    std::env::var("CASS_CACHE_TOTAL_CAP")
        .ok()
        .and_then(|v| v.parse::<usize>().ok())
        .filter(|v| *v > 0)
        .unwrap_or(2048)
});

static CACHE_DEBUG_ENABLED: Lazy<bool> = Lazy::new(|| {
    std::env::var("CASS_DEBUG_CACHE_METRICS")
        .map(|v| v == "1" || v.eq_ignore_ascii_case("true"))
        .unwrap_or(false)
});

// Optional byte-based cap for cache memory; 0 means no byte limit (entry-based only).
// Approximate sizing: ~500 bytes per cached hit typical (content/title/snippets).
// Example: CASS_CACHE_BYTE_CAP=10485760 for ~10MB limit.
static CACHE_BYTE_CAP: Lazy<usize> = Lazy::new(|| {
    std::env::var("CASS_CACHE_BYTE_CAP")
        .ok()
        .and_then(|v| v.parse::<usize>().ok())
        .unwrap_or(0) // 0 = disabled (entry-based cap only)
});

const CACHE_KEY_VERSION: &str = "1";

// Warm debounce (ms) for background reload/warm jobs; default 120ms.
static WARM_DEBOUNCE_MS: Lazy<u64> = Lazy::new(|| {
    std::env::var("CASS_WARM_DEBOUNCE_MS")
        .ok()
        .and_then(|v| v.parse::<u64>().ok())
        .filter(|v| *v > 0)
        .unwrap_or(120)
});

#[derive(Clone)]
struct CachedHit {
    hit: SearchHit,
    lc_content: String,
    lc_title: Option<String>,
    lc_snippet: String,
    bloom64: u64,
}

impl CachedHit {
    /// Approximate byte size of this cached hit (rough estimate for memory guardrails).
    /// Includes SearchHit strings + lowercase copies + bloom filter.
    fn approx_bytes(&self) -> usize {
        // Base struct overhead
        let base = std::mem::size_of::<Self>();
        // SearchHit string fields (title, snippet, content, source_path, agent, workspace)
        let hit_strings = self.hit.title.len()
            + self.hit.snippet.len()
            + self.hit.content.len()
            + self.hit.source_path.len()
            + self.hit.agent.len()
            + self.hit.workspace.len();
        // Lowercase cache copies
        let lc_strings = self.lc_content.len()
            + self.lc_title.as_ref().map(|s| s.len()).unwrap_or(0)
            + self.lc_snippet.len();
        base + hit_strings + lc_strings
    }
}

struct CacheShards {
    shards: HashMap<String, LruCache<String, Vec<CachedHit>>>,
    total_cap: usize,
    total_cost: usize,
    /// Running count of evictions (for diagnostics)
    eviction_count: u64,
    /// Approximate bytes used by all cached hits
    total_bytes: usize,
    /// Byte cap (0 = disabled)
    byte_cap: usize,
}

impl CacheShards {
    fn new(total_cap: usize, byte_cap: usize) -> Self {
        Self {
            shards: HashMap::new(),
            total_cap: total_cap.max(1),
            total_cost: 0,
            eviction_count: 0,
            total_bytes: 0,
            byte_cap,
        }
    }

    fn shard_mut(&mut self, name: &str) -> &mut LruCache<String, Vec<CachedHit>> {
        self.shards
            .entry(name.to_string())
            .or_insert_with(|| LruCache::new(NonZeroUsize::new(*CACHE_SHARD_CAP).unwrap()))
    }

    fn shard_opt(&self, name: &str) -> Option<&LruCache<String, Vec<CachedHit>>> {
        self.shards.get(name)
    }

    fn put(&mut self, shard_name: &str, key: String, value: Vec<CachedHit>) {
        let shard = self.shard_mut(shard_name);
        let new_cost = value.len();
        let new_bytes: usize = value.iter().map(|h| h.approx_bytes()).sum();
        // Subtract old entry's cost/bytes if replacing
        let (old_cost, old_bytes) = shard
            .get(&key)
            .map(|v| (v.len(), v.iter().map(|h| h.approx_bytes()).sum()))
            .unwrap_or((0, 0));
        shard.put(key, value);
        self.total_cost += new_cost.saturating_sub(old_cost);
        self.total_bytes += new_bytes.saturating_sub(old_bytes);
        self.evict_until_within_cap();
    }

    fn evict_until_within_cap(&mut self) {
        // Evict if over entry cap OR over byte cap (when byte_cap > 0)
        while self.total_cost > self.total_cap
            || (self.byte_cap > 0 && self.total_bytes > self.byte_cap)
        {
            let mut evicted = false;
            for shard in self.shards.values_mut() {
                if let Some((_k, v)) = shard.pop_lru() {
                    let evicted_bytes: usize = v.iter().map(|h| h.approx_bytes()).sum();
                    self.total_cost = self.total_cost.saturating_sub(v.len());
                    self.total_bytes = self.total_bytes.saturating_sub(evicted_bytes);
                    self.eviction_count += 1;
                    evicted = true;
                    // Check if we're back within both caps
                    let within_cost = self.total_cost <= self.total_cap;
                    let within_bytes = self.byte_cap == 0 || self.total_bytes <= self.byte_cap;
                    if within_cost && within_bytes {
                        break;
                    }
                }
            }
            if !evicted {
                break;
            }
        }
    }

    fn clear(&mut self) {
        self.shards.clear();
        self.total_cost = 0;
        self.total_bytes = 0;
        // Note: eviction_count preserved for lifetime stats
    }

    fn total_cost(&self) -> usize {
        self.total_cost
    }

    fn total_cap(&self) -> usize {
        self.total_cap
    }

    fn eviction_count(&self) -> u64 {
        self.eviction_count
    }

    fn total_bytes(&self) -> usize {
        self.total_bytes
    }

    fn byte_cap(&self) -> usize {
        self.byte_cap
    }
}

#[derive(Clone)]
struct WarmJob {
    query: String,
    _filters: SearchFilters,
}

#[derive(Clone)]
struct SearcherCacheEntry {
    epoch: u64,
    searcher: Searcher,
}

thread_local! {
    static THREAD_SEARCHER: RefCell<Option<SearcherCacheEntry>> = const { RefCell::new(None) };
}

fn sanitize_query(raw: &str) -> String {
    // Replace any character that is not alphanumeric or asterisk with a space.
    // Asterisks are preserved for wildcard query support (*foo, foo*, *bar*).
    // This ensures that the input tokens match how SimpleTokenizer splits content.
    // e.g. "c++" -> "c  ", "foo.bar" -> "foo bar", "*config*" -> "*config*"
    raw.chars()
        .map(|c| {
            if c.is_alphanumeric() || c == '*' {
                c
            } else {
                ' '
            }
        })
        .collect()
}

/// Calculate Levenshtein edit distance between two strings.
/// Used for typo detection in did-you-mean suggestions.
fn levenshtein_distance(a: &str, b: &str) -> usize {
    let a_chars: Vec<char> = a.chars().collect();
    let b_chars: Vec<char> = b.chars().collect();
    let a_len = a_chars.len();
    let b_len = b_chars.len();

    if a_len == 0 {
        return b_len;
    }
    if b_len == 0 {
        return a_len;
    }

    // Use two rows for space efficiency
    let mut prev_row: Vec<usize> = (0..=b_len).collect();
    let mut curr_row: Vec<usize> = vec![0; b_len + 1];

    for (i, a_char) in a_chars.iter().enumerate() {
        curr_row[0] = i + 1;
        for (j, b_char) in b_chars.iter().enumerate() {
            let cost = if a_char == b_char { 0 } else { 1 };
            curr_row[j + 1] = (prev_row[j + 1] + 1) // deletion
                .min(curr_row[j] + 1) // insertion
                .min(prev_row[j] + cost); // substitution
        }
        std::mem::swap(&mut prev_row, &mut curr_row);
    }

    prev_row[b_len]
}

/// Escape special regex characters in a string
fn escape_regex(s: &str) -> String {
    let mut escaped = String::with_capacity(s.len() * 2);
    for c in s.chars() {
        match c {
            '\\' | '.' | '+' | '*' | '?' | '(' | ')' | '[' | ']' | '{' | '}' | '|' | '^' | '$' => {
                escaped.push('\\');
                escaped.push(c);
            }
            _ => escaped.push(c),
        }
    }
    escaped
}

/// Represents different wildcard patterns for a search term
#[derive(Debug, Clone, PartialEq)]
enum WildcardPattern {
    /// No wildcards - exact term match (through edge n-grams)
    Exact(String),
    /// Trailing wildcard: foo* (prefix match)
    Prefix(String),
    /// Leading wildcard: *foo (suffix match - requires regex)
    Suffix(String),
    /// Both wildcards: *foo* (substring match - requires regex)
    Substring(String),
}

impl WildcardPattern {
    fn parse(term: &str) -> Self {
        let starts_with_star = term.starts_with('*');
        let ends_with_star = term.ends_with('*');

        let core = term.trim_matches('*').to_lowercase();
        if core.is_empty() {
            return WildcardPattern::Exact(String::new());
        }

        match (starts_with_star, ends_with_star) {
            (true, true) => WildcardPattern::Substring(core),
            (true, false) => WildcardPattern::Suffix(core),
            (false, true) => WildcardPattern::Prefix(core),
            (false, false) => WildcardPattern::Exact(core),
        }
    }

    /// Convert to regex pattern for Tantivy RegexQuery
    fn to_regex(&self) -> Option<String> {
        match self {
            WildcardPattern::Suffix(core) => Some(format!(".*{}", escape_regex(core))),
            WildcardPattern::Substring(core) => Some(format!(".*{}.*", escape_regex(core))),
            _ => None,
        }
    }

    /// Convert to the corresponding public MatchType
    fn to_match_type(&self) -> MatchType {
        match self {
            WildcardPattern::Exact(_) => MatchType::Exact,
            WildcardPattern::Prefix(_) => MatchType::Prefix,
            WildcardPattern::Suffix(_) => MatchType::Suffix,
            WildcardPattern::Substring(_) => MatchType::Substring,
        }
    }
}

/// Token types for boolean query parsing
#[derive(Debug, Clone, PartialEq)]
enum QueryToken {
    /// A search term (may include wildcards)
    Term(String),
    /// Quoted phrase for exact matching
    Phrase(String),
    /// AND operator (explicit)
    And,
    /// OR operator
    Or,
    /// NOT operator (next term is excluded)
    Not,
}

/// Parse a query string into boolean tokens.
/// Supports:
/// - AND, && for explicit AND (implicit between terms)
/// - OR, || for OR
/// - NOT, - prefix for exclusion
/// - "quoted phrases" for exact matching
fn parse_boolean_query(query: &str) -> Vec<QueryToken> {
    let mut tokens = Vec::new();
    let mut chars = query.chars().peekable();
    let mut current_word = String::new();

    while let Some(c) = chars.next() {
        match c {
            '"' => {
                // Flush any pending word
                if !current_word.is_empty() {
                    tokens.push(QueryToken::Term(std::mem::take(&mut current_word)));
                }
                // Collect quoted phrase
                let mut phrase = String::new();
                while let Some(&next) = chars.peek() {
                    if next == '"' {
                        chars.next();
                        break;
                    }
                    phrase.push(chars.next().unwrap());
                }
                if !phrase.is_empty() {
                    tokens.push(QueryToken::Phrase(phrase));
                }
            }
            '&' if chars.peek() == Some(&'&') => {
                chars.next(); // consume second &
                if !current_word.is_empty() {
                    tokens.push(QueryToken::Term(std::mem::take(&mut current_word)));
                }
                tokens.push(QueryToken::And);
            }
            '|' if chars.peek() == Some(&'|') => {
                chars.next(); // consume second |
                if !current_word.is_empty() {
                    tokens.push(QueryToken::Term(std::mem::take(&mut current_word)));
                }
                tokens.push(QueryToken::Or);
            }
            '-' if current_word.is_empty() => {
                // Prefix minus for NOT (at start of a term)
                // Works at query start: "-foo" or mid-query: "bar -foo"
                tokens.push(QueryToken::Not);
            }
            ' ' | '\t' | '\n' => {
                if !current_word.is_empty() {
                    let word = std::mem::take(&mut current_word);
                    let upper = word.to_uppercase();
                    match upper.as_str() {
                        "AND" => tokens.push(QueryToken::And),
                        "OR" => tokens.push(QueryToken::Or),
                        "NOT" => tokens.push(QueryToken::Not),
                        _ => tokens.push(QueryToken::Term(word)),
                    }
                }
            }
            _ => {
                current_word.push(c);
            }
        }
    }

    // Flush final word
    if !current_word.is_empty() {
        let upper = current_word.to_uppercase();
        match upper.as_str() {
            "AND" => tokens.push(QueryToken::And),
            "OR" => tokens.push(QueryToken::Or),
            "NOT" => tokens.push(QueryToken::Not),
            _ => tokens.push(QueryToken::Term(current_word)),
        }
    }

    tokens
}

/// Check if a query string contains boolean operators
fn has_boolean_operators(query: &str) -> bool {
    let tokens = parse_boolean_query(query);
    tokens.iter().any(|t| {
        matches!(
            t,
            QueryToken::And | QueryToken::Or | QueryToken::Not | QueryToken::Phrase(_)
        )
    })
}

/// Build Tantivy query clauses from boolean tokens.
/// Returns clauses for use in a BooleanQuery.
fn build_boolean_query_clauses(
    tokens: &[QueryToken],
    fields: &crate::search::tantivy::Fields,
) -> Vec<(Occur, Box<dyn Query>)> {
    let mut clauses: Vec<(Occur, Box<dyn Query>)> = Vec::new();
    let mut pending_or_group: Vec<Box<dyn Query>> = Vec::new();
    let mut next_occur = Occur::Must;
    let mut in_or_sequence = false;

    for token in tokens {
        match token {
            QueryToken::And => {
                // Flush any OR group
                if !pending_or_group.is_empty() {
                    let or_clauses: Vec<_> = pending_or_group
                        .drain(..)
                        .map(|q| (Occur::Should, q))
                        .collect();
                    clauses.push((Occur::Must, Box::new(BooleanQuery::new(or_clauses))));
                }
                in_or_sequence = false;
                next_occur = Occur::Must;
            }
            QueryToken::Or => {
                in_or_sequence = true;
                // Don't change next_occur; OR will group with previous term
            }
            QueryToken::Not => {
                // Flush any OR group
                if !pending_or_group.is_empty() {
                    let or_clauses: Vec<_> = pending_or_group
                        .drain(..)
                        .map(|q| (Occur::Should, q))
                        .collect();
                    clauses.push((Occur::Must, Box::new(BooleanQuery::new(or_clauses))));
                }
                in_or_sequence = false;
                next_occur = Occur::MustNot;
            }
            QueryToken::Term(term) => {
                let pattern = WildcardPattern::parse(term);
                let term_shoulds = build_term_query_clauses(&pattern, fields);
                if term_shoulds.is_empty() {
                    continue;
                }
                let term_query: Box<dyn Query> = Box::new(BooleanQuery::new(term_shoulds));

                if in_or_sequence || next_occur == Occur::Should {
                    // Add to OR group
                    if pending_or_group.is_empty() {
                        // Pull last Must clause into OR group if exists
                        if let Some((Occur::Must, last_q)) = clauses.pop() {
                            pending_or_group.push(last_q);
                        }
                    }
                    pending_or_group.push(term_query);
                    in_or_sequence = true; // Continue OR sequence
                } else {
                    clauses.push((next_occur, term_query));
                }
                next_occur = Occur::Must; // Reset for next term
            }
            QueryToken::Phrase(phrase) => {
                // For phrases, search all words as MUST within the phrase
                let words: Vec<&str> = phrase.split_whitespace().collect();
                if words.is_empty() {
                    continue;
                }
                let mut phrase_clauses: Vec<(Occur, Box<dyn Query>)> = Vec::new();
                for word in words {
                    let pattern = WildcardPattern::parse(word);
                    let term_shoulds = build_term_query_clauses(&pattern, fields);
                    if !term_shoulds.is_empty() {
                        phrase_clauses
                            .push((Occur::Must, Box::new(BooleanQuery::new(term_shoulds))));
                    }
                }
                if phrase_clauses.is_empty() {
                    continue;
                }
                let phrase_query: Box<dyn Query> = Box::new(BooleanQuery::new(phrase_clauses));

                if in_or_sequence {
                    if pending_or_group.is_empty()
                        && let Some((Occur::Must, last_q)) = clauses.pop()
                    {
                        pending_or_group.push(last_q);
                    }
                    pending_or_group.push(phrase_query);
                } else {
                    clauses.push((next_occur, phrase_query));
                }
                next_occur = Occur::Must;
            }
        }
    }

    // Flush any remaining OR group
    if !pending_or_group.is_empty() {
        let or_clauses: Vec<_> = pending_or_group
            .drain(..)
            .map(|q| (Occur::Should, q))
            .collect();
        clauses.push((Occur::Must, Box::new(BooleanQuery::new(or_clauses))));
    }

    clauses
}

/// Determine the dominant match type from a query string.
/// Returns the "loosest" pattern used (Substring > Suffix > Prefix > Exact).
fn dominant_match_type(query: &str) -> MatchType {
    let terms: Vec<&str> = query.split_whitespace().collect();
    if terms.is_empty() {
        return MatchType::Exact;
    }

    let mut worst = MatchType::Exact;
    for term in terms {
        let pattern = WildcardPattern::parse(term);
        let mt = pattern.to_match_type();
        // Lower quality factor = "looser" match = dominant
        if mt.quality_factor() < worst.quality_factor() {
            worst = mt;
        }
    }
    worst
}

/// Build query clauses for a single term based on its wildcard pattern.
/// Returns a Vec of (Occur::Should, Query) for use in a BooleanQuery.
fn build_term_query_clauses(
    pattern: &WildcardPattern,
    fields: &crate::search::tantivy::Fields,
) -> Vec<(Occur, Box<dyn Query>)> {
    let mut shoulds: Vec<(Occur, Box<dyn Query>)> = Vec::new();

    match pattern {
        WildcardPattern::Exact(term) | WildcardPattern::Prefix(term) => {
            // For exact and prefix patterns, use TermQuery on all fields
            // (edge n-grams already handle prefix matching)
            if term.is_empty() {
                return shoulds;
            }
            shoulds.push((
                Occur::Should,
                Box::new(TermQuery::new(
                    Term::from_field_text(fields.title, term),
                    IndexRecordOption::WithFreqsAndPositions,
                )),
            ));
            shoulds.push((
                Occur::Should,
                Box::new(TermQuery::new(
                    Term::from_field_text(fields.content, term),
                    IndexRecordOption::WithFreqsAndPositions,
                )),
            ));
            shoulds.push((
                Occur::Should,
                Box::new(TermQuery::new(
                    Term::from_field_text(fields.title_prefix, term),
                    IndexRecordOption::WithFreqsAndPositions,
                )),
            ));
            shoulds.push((
                Occur::Should,
                Box::new(TermQuery::new(
                    Term::from_field_text(fields.content_prefix, term),
                    IndexRecordOption::WithFreqsAndPositions,
                )),
            ));
        }
        WildcardPattern::Suffix(term) | WildcardPattern::Substring(term) => {
            // For suffix and substring patterns, use RegexQuery
            if term.is_empty() {
                return shoulds;
            }
            if let Some(regex_pattern) = pattern.to_regex() {
                // Try to create RegexQuery for content field
                if let Ok(rq) = RegexQuery::from_pattern(&regex_pattern, fields.content) {
                    shoulds.push((Occur::Should, Box::new(rq)));
                }
                // Also try for title field
                if let Ok(rq) = RegexQuery::from_pattern(&regex_pattern, fields.title) {
                    shoulds.push((Occur::Should, Box::new(rq)));
                }
            }
        }
    }

    shoulds
}

/// Check if content is primarily a tool invocation (noise that shouldn't appear in search results).
/// Tool invocations like "[Tool: Bash - Check status]" are not informative search results.
fn is_tool_invocation_noise(content: &str) -> bool {
    let trimmed = content.trim();

    // Direct tool invocations that are just "[Tool: X - description]"
    if trimmed.starts_with("[Tool:") {
        // If it's short or ends with ']', it's pure noise
        if trimmed.len() < 100 || trimmed.ends_with(']') {
            return true;
        }
    }

    // Also filter very short content that's just tool names or markers
    if trimmed.len() < 20 {
        let lower = trimmed.to_lowercase();
        if lower.starts_with("[tool") || lower.starts_with("tool:") {
            return true;
        }
    }

    false
}

/// Deduplicate search hits by content, keeping only the highest-scored hit for each unique content.
/// This removes duplicate results when the same message appears multiple times (e.g., user repeated
/// themselves in a conversation, or the same content was indexed from multiple sources).
/// Also filters out tool invocation noise that isn't useful for search results.
fn deduplicate_hits(hits: Vec<SearchHit>) -> Vec<SearchHit> {
    let mut seen: HashMap<String, usize> = HashMap::new();
    let mut deduped: Vec<SearchHit> = Vec::new();

    for hit in hits {
        // Skip tool invocation noise
        if is_tool_invocation_noise(&hit.content) {
            continue;
        }

        // Normalize content for comparison (trim whitespace, collapse multiple spaces)
        let normalized = hit.content.split_whitespace().collect::<Vec<_>>().join(" ");

        if let Some(&existing_idx) = seen.get(&normalized) {
            // If existing hit has lower score, replace it
            if deduped[existing_idx].score < hit.score {
                deduped[existing_idx] = hit;
            }
            // Otherwise keep existing (higher score)
        } else {
            seen.insert(normalized, deduped.len());
            deduped.push(hit);
        }
    }

    deduped
}

impl SearchClient {
    pub fn open(index_path: &Path, db_path: Option<&Path>) -> Result<Option<Self>> {
        let tantivy = Index::open_in_dir(index_path).ok().and_then(|mut idx| {
            // Register custom tokenizer so searches work
            crate::search::tantivy::ensure_tokenizer(&mut idx);
            let schema = idx.schema();
            let fields = fields_from_schema(&schema).ok()?;
            idx.reader().ok().map(|reader| (reader, fields))
        });

        let sqlite = db_path.and_then(|p| Connection::open(p).ok());

        if tantivy.is_none() && sqlite.is_none() {
            return Ok(None);
        }

        let shared_filters = Arc::new(Mutex::new(()));
        let reload_epoch = Arc::new(AtomicU64::new(0));
        let metrics = Metrics::default();
        let cache_namespace = format!(
            "v{}|schema:{}",
            CACHE_KEY_VERSION,
            crate::search::tantivy::SCHEMA_HASH
        );

        let warm_pair = if let Some((reader, fields)) = &tantivy {
            maybe_spawn_warm_worker(
                reader.clone(),
                *fields,
                Arc::downgrade(&shared_filters),
                reload_epoch.clone(),
                metrics.clone(),
            )
        } else {
            None
        };

        Ok(Some(Self {
            reader: tantivy,
            sqlite,
            prefix_cache: Mutex::new(CacheShards::new(*CACHE_TOTAL_CAP, *CACHE_BYTE_CAP)),
            last_reload: Mutex::new(None),
            last_generation: Mutex::new(None),
            reload_epoch,
            warm_tx: warm_pair.as_ref().map(|(tx, _)| tx.clone()),
            _warm_handle: warm_pair.map(|(_, h)| h),
            _shared_filters: shared_filters,
            metrics,
            cache_namespace,
        }))
    }

    pub fn search(
        &self,
        query: &str,
        filters: SearchFilters,
        limit: usize,
        offset: usize,
    ) -> Result<Vec<SearchHit>> {
        let sanitized = sanitize_query(query);

        // Schedule warmup for likely prefixes when user pauses typing.
        if offset == 0
            && let Some(tx) = &self.warm_tx
        {
            let _ = tx.send(WarmJob {
                query: sanitized.clone(),
                _filters: filters.clone(),
            });
        }

        // Fast path: reuse cached prefix when user is typing forward (offset 0 only).
        if offset == 0 {
            if let Some(cached) = self.cached_prefix_hits(&sanitized, &filters) {
                let mut filtered: Vec<SearchHit> = cached
                    .into_iter()
                    .filter(|h| hit_matches_query_cached(h, &sanitized))
                    .map(|c| c.hit.clone())
                    .collect();
                if filtered.len() >= limit {
                    filtered.truncate(limit);
                    self.metrics.inc_cache_hits();
                    self.maybe_log_cache_metrics("hit");
                    return Ok(filtered);
                }
                self.metrics.inc_cache_shortfall();
                self.maybe_log_cache_metrics("shortfall");
            }
            self.metrics.inc_cache_miss();
            self.maybe_log_cache_metrics("miss");
        }

        // Tantivy is the primary high-performance engine.
        if let Some((reader, fields)) = &self.reader {
            tracing::info!(
                backend = "tantivy",
                query = sanitized,
                limit = limit,
                offset = offset,
                "search_start"
            );
            let hits = self.search_tantivy(
                reader,
                fields,
                &sanitized,
                filters.clone(),
                limit * 3,
                offset,
            )?;
            if !hits.is_empty() {
                let mut deduped = deduplicate_hits(hits);
                deduped.truncate(limit);
                self.put_cache(&sanitized, &filters, &deduped);
                return Ok(deduped);
            }
            // If Tantivy yields 0 results, we can optionally fall back to SQLite FTS
            // if we suspect consistency issues, but for now let's trust Tantivy
            // or fall through if you prefer robust fallback.
            // Given the "speed first" requirement, we return early if we got hits.
            // If empty, we *can* try SQLite just in case index is lagging.
        }

        // Fallback: SQLite FTS (slower, but strictly consistent with DB)
        // Skip SQLite fallback when the query contains leading/trailing wildcards that
        // FTS5 cannot parse (e.g., "*handler" or "*foo*"), to avoid "unknown special query" errors.
        let query_has_wildcards = sanitized.contains('*');
        if let Some(conn) = &self.sqlite {
            if query_has_wildcards {
                return Ok(Vec::new());
            }
            tracing::info!(
                backend = "sqlite",
                query = sanitized,
                limit = limit,
                offset = offset,
                "search_start"
            );
            let hits = self.search_sqlite(conn, &sanitized, filters.clone(), limit * 3, offset)?;
            let mut deduped = deduplicate_hits(hits);
            deduped.truncate(limit);
            self.put_cache(&sanitized, &filters, &deduped);
            return Ok(deduped);
        }

        tracing::info!(backend = "none", query = query, "search_start");
        Ok(Vec::new())
    }

    /// Search with automatic wildcard fallback for sparse results.
    /// If the initial search returns fewer than `sparse_threshold` results and the query
    /// doesn't already contain wildcards, automatically retry with substring wildcards (*term*).
    pub fn search_with_fallback(
        &self,
        query: &str,
        filters: SearchFilters,
        limit: usize,
        offset: usize,
        sparse_threshold: usize,
    ) -> Result<SearchResult> {
        // First, try the normal search
        let hits = self.search(query, filters.clone(), limit, offset)?;
        let baseline_stats = self.cache_stats();

        // Check if we should try wildcard fallback
        let query_has_wildcards = query.contains('*');
        let is_sparse = hits.len() < sparse_threshold && offset == 0;

        if !is_sparse || query_has_wildcards || query.trim().is_empty() {
            // Either we have enough results, query already has wildcards, or query is empty
            // Generate suggestions only if truly zero hits
            let suggestions = if hits.is_empty() && !query.trim().is_empty() {
                self.generate_suggestions(query, &filters)
            } else {
                Vec::new()
            };
            return Ok(SearchResult {
                hits,
                wildcard_fallback: false,
                cache_stats: baseline_stats,
                suggestions,
            });
        }

        // Try wildcard fallback: wrap each term in *term*
        let wildcard_query = query
            .split_whitespace()
            .map(|term| format!("*{}*", term.trim_matches('*')))
            .collect::<Vec<_>>()
            .join(" ");

        tracing::info!(
            original_query = query,
            wildcard_query = wildcard_query,
            original_count = hits.len(),
            "wildcard_fallback"
        );

        let mut fallback_hits = self.search(&wildcard_query, filters.clone(), limit, offset)?;
        let fallback_stats = self.cache_stats();

        // Use fallback results if they're better
        if fallback_hits.len() > hits.len() {
            // Mark all hits as ImplicitWildcard since we auto-added wildcards
            for hit in &mut fallback_hits {
                hit.match_type = MatchType::ImplicitWildcard;
            }
            // Generate suggestions if still zero hits after fallback
            let suggestions = if fallback_hits.is_empty() {
                self.generate_suggestions(query, &filters)
            } else {
                Vec::new()
            };
            Ok(SearchResult {
                hits: fallback_hits,
                wildcard_fallback: true,
                cache_stats: fallback_stats,
                suggestions,
            })
        } else {
            // Keep original results even if sparse
            // Generate suggestions if zero hits
            let suggestions = if hits.is_empty() {
                self.generate_suggestions(query, &filters)
            } else {
                Vec::new()
            };
            Ok(SearchResult {
                hits,
                wildcard_fallback: false,
                cache_stats: baseline_stats,
                suggestions,
            })
        }
    }

    /// Generate "did-you-mean" suggestions for zero-hit queries.
    fn generate_suggestions(&self, query: &str, filters: &SearchFilters) -> Vec<QuerySuggestion> {
        let mut suggestions = Vec::new();
        let query_lower = query.to_lowercase();

        // 1. Suggest wildcard search if query doesn't have wildcards
        if !query.contains('*') && query.len() >= 2 {
            suggestions.push(QuerySuggestion::wildcard(query).with_shortcut(1));
        }

        // 2. Suggest removing agent filter if one is set
        if !filters.agents.is_empty() {
            let agents: Vec<&str> = filters.agents.iter().map(|s| s.as_str()).collect();
            let agent_str = agents.join(", ");
            suggestions.push(QuerySuggestion::remove_agent_filter(&agent_str).with_shortcut(2));
        }

        // 3. Suggest common agent names if query looks like a typo of one
        let known_agents = [
            "codex",
            "claude",
            "claude_code",
            "cline",
            "gemini",
            "amp",
            "opencode",
        ];
        for agent in &known_agents {
            if levenshtein_distance(&query_lower, agent) <= 2 && query_lower != *agent {
                suggestions.push(
                    QuerySuggestion::spelling(query, agent)
                        .with_shortcut(suggestions.len().min(2) as u8 + 1),
                );
                break; // Only suggest one spelling fix
            }
        }

        // 4. Suggest alternative agents if we have SQLite connection and no agent filter
        if filters.agents.is_empty()
            && let Some(ref conn) = self.sqlite
            && let Ok(mut stmt) = conn
                .prepare("SELECT DISTINCT agent_slug FROM conversations ORDER BY id DESC LIMIT 3")
            && let Ok(rows) = stmt.query_map([], |row| row.get::<_, String>(0))
        {
            for row in rows.flatten() {
                if suggestions.len() < 3 {
                    suggestions.push(
                        QuerySuggestion::try_agent(&row)
                            .with_shortcut(suggestions.len().min(2) as u8 + 1),
                    );
                }
            }
        }

        // Ensure we have at most 3 suggestions with shortcuts 1, 2, 3
        suggestions.truncate(3);
        for (i, sugg) in suggestions.iter_mut().enumerate() {
            sugg.shortcut = Some((i + 1) as u8);
        }

        suggestions
    }

    fn searcher_for_thread(&self, reader: &IndexReader) -> Searcher {
        let epoch = self.reload_epoch.load(Ordering::Relaxed);
        THREAD_SEARCHER.with(|slot| {
            let mut slot = slot.borrow_mut();
            if let Some(entry) = slot.as_ref()
                && entry.epoch == epoch
            {
                return entry.searcher.clone();
            }
            let searcher = reader.searcher();
            *slot = Some(SearcherCacheEntry {
                epoch,
                searcher: searcher.clone(),
            });
            searcher
        })
    }

    fn track_generation(&self, generation: u64) {
        let mut guard = self.last_generation.lock().unwrap();
        if let Some(prev) = *guard
            && prev != generation
            && let Ok(mut cache) = self.prefix_cache.lock()
        {
            cache.clear();
        }
        *guard = Some(generation);
    }

    fn search_tantivy(
        &self,
        reader: &IndexReader,
        fields: &crate::search::tantivy::Fields,
        query: &str,
        filters: SearchFilters,
        limit: usize,
        offset: usize,
    ) -> Result<Vec<SearchHit>> {
        self.maybe_reload_reader(reader)?;
        let searcher = self.searcher_for_thread(reader);
        self.track_generation(searcher.generation().generation_id());

        let mut clauses: Vec<(Occur, Box<dyn Query>)> = Vec::new();

        // Parse query with boolean operator support (AND, OR, NOT, "phrases")
        // Falls back to simple whitespace split for plain queries (implicit AND)
        let tokens = parse_boolean_query(query);
        if tokens.is_empty() {
            clauses.push((Occur::Must, Box::new(AllQuery)));
        } else if has_boolean_operators(query) {
            // Use boolean query builder for complex queries
            let bool_clauses = build_boolean_query_clauses(&tokens, fields);
            clauses.extend(bool_clauses);
        } else {
            // Simple query: treat each term as MUST (implicit AND)
            for token in tokens {
                if let QueryToken::Term(term_str) = token {
                    let pattern = WildcardPattern::parse(&term_str);
                    let term_shoulds = build_term_query_clauses(&pattern, fields);
                    if !term_shoulds.is_empty() {
                        clauses.push((Occur::Must, Box::new(BooleanQuery::new(term_shoulds))));
                    }
                }
            }
        }

        if !filters.agents.is_empty() {
            let terms = filters
                .agents
                .into_iter()
                .map(|agent| {
                    (
                        Occur::Should,
                        Box::new(TermQuery::new(
                            Term::from_field_text(fields.agent, &agent),
                            IndexRecordOption::Basic,
                        )) as Box<dyn Query>,
                    )
                })
                .collect();
            clauses.push((Occur::Must, Box::new(BooleanQuery::new(terms))));
        }

        if !filters.workspaces.is_empty() {
            let terms = filters
                .workspaces
                .into_iter()
                .map(|ws| {
                    (
                        Occur::Should,
                        Box::new(TermQuery::new(
                            Term::from_field_text(fields.workspace, &ws),
                            IndexRecordOption::Basic,
                        )) as Box<dyn Query>,
                    )
                })
                .collect();
            clauses.push((Occur::Must, Box::new(BooleanQuery::new(terms))));
        }

        if filters.created_from.is_some() || filters.created_to.is_some() {
            use std::ops::Bound::{Included, Unbounded};
            let lower = filters
                .created_from
                .map(|v| Included(Term::from_field_i64(fields.created_at, v)))
                .unwrap_or(Unbounded);
            let upper = filters
                .created_to
                .map(|v| Included(Term::from_field_i64(fields.created_at, v)))
                .unwrap_or(Unbounded);
            let range = RangeQuery::new(lower, upper);
            clauses.push((Occur::Must, Box::new(range)));
        }

        let q: Box<dyn Query> = if clauses.is_empty() {
            Box::new(AllQuery)
        } else if clauses.len() == 1 {
            clauses.pop().unwrap().1
        } else {
            Box::new(BooleanQuery::new(clauses))
        };

        let prefix_only = is_prefix_only(query);
        let snippet_generator = if prefix_only {
            None
        } else {
            Some(SnippetGenerator::create(&searcher, &*q, fields.content)?)
        };

        let top_docs = searcher.search(&q, &TopDocs::with_limit(limit).and_offset(offset))?;
        // Compute match type once for all results (not per-hit)
        let query_match_type = dominant_match_type(query);
        let mut hits = Vec::new();
        for (score, addr) in top_docs {
            let doc: TantivyDocument = searcher.doc(addr)?;
            let title = doc
                .get_first(fields.title)
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .to_string();
            let content = doc
                .get_first(fields.content)
                .or_else(|| doc.get_first(fields.preview))
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .to_string();
            let agent = doc
                .get_first(fields.agent)
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .to_string();
            let snippet = if let Some(r#gen) = &snippet_generator {
                r#gen
                    .snippet_from_doc(&doc)
                    .to_html()
                    .replace("<b>", "**")
                    .replace("</b>", "**")
            } else if let Some(sn) = cached_prefix_snippet(&content, query, 160) {
                sn
            } else {
                quick_prefix_snippet(&content, query, 160)
            };
            let source = doc
                .get_first(fields.source_path)
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .to_string();
            let workspace = doc
                .get_first(fields.workspace)
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .to_string();
            let created_at = doc.get_first(fields.created_at).and_then(|v| v.as_i64());
            hits.push(SearchHit {
                title,
                snippet,
                content,
                score,
                source_path: source,
                agent,
                workspace,
                created_at,
                line_number: None, // TODO: populate from index if stored
                match_type: query_match_type,
            });
        }
        Ok(hits)
    }

    fn search_sqlite(
        &self,
        conn: &Connection,
        query: &str,
        filters: SearchFilters,
        limit: usize,
        offset: usize,
    ) -> Result<Vec<SearchHit>> {
        // FTS5 cannot handle empty queries
        if query.trim().is_empty() {
            return Ok(Vec::new());
        }
        // Compute match type once for all results
        let query_match_type = dominant_match_type(query);
        let mut sql = String::from(
            "SELECT f.title, f.content, f.agent, f.workspace, f.source_path, f.created_at, bm25(fts_messages) AS score, snippet(fts_messages, 0, '**', '**', '...', 64) AS snippet, m.idx
             FROM fts_messages f
             LEFT JOIN messages m ON f.message_id = m.id
             WHERE fts_messages MATCH ?",
        );
        let mut params: Vec<Box<dyn rusqlite::ToSql>> = vec![Box::new(query.to_string())];

        if !filters.agents.is_empty() {
            let placeholders = (0..filters.agents.len())
                .map(|_| "?".to_string())
                .collect::<Vec<_>>()
                .join(",");
            sql.push_str(&format!(" AND f.agent IN ({placeholders})"));
            for a in filters.agents {
                params.push(Box::new(a));
            }
        }

        if !filters.workspaces.is_empty() {
            let placeholders = (0..filters.workspaces.len())
                .map(|_| "?".to_string())
                .collect::<Vec<_>>()
                .join(",");
            sql.push_str(&format!(" AND f.workspace IN ({placeholders})"));
            for w in filters.workspaces {
                params.push(Box::new(w));
            }
        }

        if let Some(created_from) = filters.created_from {
            sql.push_str(" AND f.created_at >= ?");
            params.push(Box::new(created_from));
        }
        if let Some(created_to) = filters.created_to {
            sql.push_str(" AND f.created_at <= ?");
            params.push(Box::new(created_to));
        }

        sql.push_str(" ORDER BY score LIMIT ? OFFSET ?");
        params.push(Box::new(limit as i64));
        params.push(Box::new(offset as i64));

        let mut stmt = conn.prepare(&sql)?;
        let rows = stmt.query_map(
            rusqlite::params_from_iter(params.iter().map(|b| &**b)),
            |row| {
                let title: String = row.get(0)?;
                let content: String = row.get(1)?;
                let agent: String = row.get(2)?;
                let workspace: String = row.get(3)?;
                let source_path: String = row.get(4)?;
                let created_at: Option<i64> = row.get(5).ok();
                let score: f32 = row.get::<_, f64>(6)? as f32;
                let snippet: String = row.get(7)?;
                // idx is 0-indexed message index; convert to 1-indexed line number for JSONL files
                let idx: Option<i64> = row.get(8).ok();
                let line_number = idx.map(|i| (i + 1) as usize);
                Ok(SearchHit {
                    title,
                    snippet,
                    content,
                    score,
                    source_path,
                    agent,
                    workspace,
                    created_at,
                    line_number,
                    match_type: query_match_type,
                })
            },
        )?;

        let mut hits = Vec::new();
        for row in rows {
            hits.push(row?);
        }
        Ok(hits)
    }
}

#[derive(Default, Clone)]
struct Metrics {
    cache_hits: Arc<Mutex<u64>>,
    cache_miss: Arc<Mutex<u64>>,
    cache_shortfall: Arc<Mutex<u64>>,
    reloads: Arc<Mutex<u64>>,
    reload_ms_total: Arc<Mutex<u128>>,
}

impl Metrics {
    fn inc_cache_hits(&self) {
        *self.cache_hits.lock().unwrap() += 1;
    }
    fn inc_cache_miss(&self) {
        *self.cache_miss.lock().unwrap() += 1;
    }
    fn inc_cache_shortfall(&self) {
        *self.cache_shortfall.lock().unwrap() += 1;
    }
    fn inc_reload(&self) {
        *self.reloads.lock().unwrap() += 1;
    }
    fn record_reload(&self, duration: Duration) {
        self.inc_reload();
        *self.reload_ms_total.lock().unwrap() += duration.as_millis();
    }

    fn snapshot_all(&self) -> (u64, u64, u64, u64, u128) {
        (
            *self.cache_hits.lock().unwrap(),
            *self.cache_miss.lock().unwrap(),
            *self.cache_shortfall.lock().unwrap(),
            *self.reloads.lock().unwrap(),
            *self.reload_ms_total.lock().unwrap(),
        )
    }

    #[cfg(test)]
    fn snapshot(&self) -> (u64, u64, u64, u64) {
        (
            *self.cache_hits.lock().unwrap(),
            *self.cache_miss.lock().unwrap(),
            *self.cache_shortfall.lock().unwrap(),
            *self.reloads.lock().unwrap(),
        )
    }
}

fn maybe_spawn_warm_worker(
    reader: IndexReader,
    fields: crate::search::tantivy::Fields,
    filters_guard: std::sync::Weak<Mutex<()>>,
    reload_epoch: Arc<AtomicU64>,
    metrics: Metrics,
) -> Option<(mpsc::UnboundedSender<WarmJob>, JoinHandle<()>)> {
    // Only spawn if a Tokio runtime is available (tests may call without one).
    if Handle::try_current().is_err() {
        return None;
    }

    let (tx, mut rx) = mpsc::unbounded_channel::<WarmJob>();
    let handle = tokio::spawn(async move {
        // Simple debounce: process at most one warmup every WARM_DEBOUNCE_MS.
        let mut last_run = Instant::now();
        while let Some(job) = rx.recv().await {
            let now = Instant::now();
            if now.duration_since(last_run) < Duration::from_millis(*WARM_DEBOUNCE_MS) {
                continue;
            }
            last_run = now;
            if filters_guard.upgrade().is_none() {
                break;
            }
            let reload_started = Instant::now();
            if let Err(err) = reader.reload() {
                tracing::warn!(error = ?err, "warm_worker_reload_failed");
                continue;
            }
            let elapsed = reload_started.elapsed();
            let epoch = reload_epoch.fetch_add(1, Ordering::SeqCst) + 1;
            metrics.record_reload(elapsed);
            tracing::debug!(
                duration_ms = elapsed.as_millis() as u64,
                reload_epoch = epoch,
                "warm_worker_reload"
            );
            // Run a tiny warm search to prefill OS cache and hit the Tantivy reader
            // without allocating full result sets. Limit 1 doc.
            let searcher = reader.searcher();
            let mut clauses: Vec<(Occur, Box<dyn Query>)> = Vec::new();
            for term_str in job.query.split_whitespace() {
                let term_lower = term_str.to_lowercase();
                let term_shoulds: Vec<(Occur, Box<dyn Query>)> = vec![
                    (
                        Occur::Should,
                        Box::new(TermQuery::new(
                            Term::from_field_text(fields.title, &term_lower),
                            IndexRecordOption::WithFreqsAndPositions,
                        )),
                    ),
                    (
                        Occur::Should,
                        Box::new(TermQuery::new(
                            Term::from_field_text(fields.content, &term_lower),
                            IndexRecordOption::WithFreqsAndPositions,
                        )),
                    ),
                ];
                clauses.push((Occur::Must, Box::new(BooleanQuery::new(term_shoulds))));
            }
            if !clauses.is_empty() {
                let q: Box<dyn Query> = Box::new(BooleanQuery::new(clauses));
                let _ = searcher.search(&q, &TopDocs::with_limit(1));
            }
        }
    });
    Some((tx, handle))
}

fn cached_hit_from(hit: &SearchHit) -> CachedHit {
    let lc_content = hit.content.to_lowercase();
    let lc_title = (!hit.title.is_empty()).then(|| hit.title.to_lowercase());
    let lc_snippet = hit.snippet.to_lowercase();
    let bloom64 = bloom_from_text(&lc_content, &lc_title, &lc_snippet);
    CachedHit {
        hit: hit.clone(),
        lc_content,
        lc_title,
        lc_snippet,
        bloom64,
    }
}

fn bloom_from_text(content: &str, title: &Option<String>, snippet: &str) -> u64 {
    let mut bits = 0u64;
    for token in token_stream(content) {
        bits |= hash_token(token);
    }
    if let Some(t) = title {
        for token in token_stream(t) {
            bits |= hash_token(token);
        }
    }
    for token in token_stream(snippet) {
        bits |= hash_token(token);
    }
    bits
}

fn token_stream(text: &str) -> impl Iterator<Item = &str> {
    text.split(|c: char| !c.is_alphanumeric())
        .filter(|s| !s.is_empty())
}

fn hash_token(tok: &str) -> u64 {
    // Simple 64-bit djb2-style hash mapped to bit position 0..63
    let mut h: u64 = 5381;
    for b in tok.as_bytes() {
        h = ((h << 5).wrapping_add(h)).wrapping_add(*b as u64);
    }
    1u64 << (h % 64)
}

fn hit_matches_query_cached(hit: &CachedHit, query: &str) -> bool {
    if query.is_empty() {
        return true;
    }
    let q = query.to_lowercase();
    let tokens: Vec<&str> = token_stream(&q).collect();
    // Bloom gate: all query tokens must have bits set
    for t in &tokens {
        let bit = hash_token(t);
        if hit.bloom64 & bit == 0 {
            return false;
        }
    }

    // Fallback substring checks on lowered fields
    hit.lc_content.contains(&q)
        || hit
            .lc_title
            .as_ref()
            .map(|t: &String| t.contains(&q))
            .unwrap_or(false)
        || hit.lc_snippet.contains(&q)
}

fn is_prefix_only(query: &str) -> bool {
    let tokens: Vec<&str> = query.split_whitespace().collect();
    if tokens.is_empty() {
        return false;
    }
    tokens
        .iter()
        .all(|t| !t.is_empty() && t.chars().all(|c| c.is_alphanumeric()))
}

fn quick_prefix_snippet(content: &str, query: &str, max_chars: usize) -> String {
    let lc_content = content.to_lowercase();
    let lc_query = query.to_lowercase();
    let content_char_count = content.chars().count();
    if let Some(pos) = lc_content.find(&lc_query) {
        // convert byte index to char index
        let start_char = content[..pos].chars().count().saturating_sub(15);
        let snippet: String = content.chars().skip(start_char).take(max_chars).collect();
        // Check if we truncated: snippet covers chars [start_char, start_char + snippet_len)
        let snippet_char_count = snippet.chars().count();
        if start_char + snippet_char_count < content_char_count {
            format!("{snippet}…")
        } else {
            snippet
        }
    } else {
        let snippet: String = content.chars().take(max_chars).collect();
        if content_char_count > max_chars {
            format!("{snippet}…")
        } else {
            snippet
        }
    }
}

fn cached_prefix_snippet(content: &str, query: &str, max_chars: usize) -> Option<String> {
    if query.trim().is_empty() {
        return None;
    }
    let lc_content = content.to_lowercase();
    let lc_query = query.to_lowercase();
    let content_char_count = content.chars().count();
    lc_content.find(&lc_query).map(|pos| {
        let start_char = content[..pos].chars().count().saturating_sub(15);
        let snippet: String = content.chars().skip(start_char).take(max_chars).collect();
        // Check if we truncated: snippet covers chars [start_char, start_char + snippet_len)
        let snippet_char_count = snippet.chars().count();
        if start_char + snippet_char_count < content_char_count {
            format!("{snippet}…")
        } else {
            snippet
        }
    })
}

fn filters_fingerprint(filters: &SearchFilters) -> String {
    let mut parts = Vec::new();
    if !filters.agents.is_empty() {
        let mut v: Vec<_> = filters.agents.iter().cloned().collect();
        v.sort();
        parts.push(format!("a:{:?}", v));
    }
    if !filters.workspaces.is_empty() {
        let mut v: Vec<_> = filters.workspaces.iter().cloned().collect();
        v.sort();
        parts.push(format!("w:{:?}", v));
    }
    if let Some(f) = filters.created_from {
        parts.push(format!("from:{f}"));
    }
    if let Some(t) = filters.created_to {
        parts.push(format!("to:{t}"));
    }
    parts.join("|")
}

impl SearchClient {
    fn maybe_reload_reader(&self, reader: &IndexReader) -> Result<()> {
        const MIN_RELOAD_INTERVAL: Duration = Duration::from_millis(300);
        let now = Instant::now();
        let mut guard = self.last_reload.lock().unwrap();
        if guard
            .map(|t| now.duration_since(t) >= MIN_RELOAD_INTERVAL)
            .unwrap_or(true)
        {
            let reload_started = Instant::now();
            reader.reload()?;
            let elapsed = reload_started.elapsed();
            *guard = Some(now);
            let epoch = self.reload_epoch.fetch_add(1, Ordering::SeqCst) + 1;
            self.metrics.record_reload(elapsed);
            tracing::debug!(
                duration_ms = elapsed.as_millis() as u64,
                reload_epoch = epoch,
                "tantivy_reader_reload"
            );
        }
        Ok(())
    }

    fn maybe_log_cache_metrics(&self, event: &str) {
        if !*CACHE_DEBUG_ENABLED {
            return;
        }
        let stats = self.cache_stats();
        tracing::debug!(
            event,
            hits = stats.cache_hits,
            miss = stats.cache_miss,
            shortfall = stats.cache_shortfall,
            reloads = stats.reloads,
            reload_ms_total = stats.reload_ms_total,
            total_cap = stats.total_cap,
            total_cost = stats.total_cost,
            evictions = stats.eviction_count,
            approx_bytes = stats.approx_bytes,
            byte_cap = stats.byte_cap,
            "cache_metrics"
        );
    }

    fn cache_key(&self, query: &str, filters: &SearchFilters) -> String {
        format!(
            "{}|{}::{}",
            self.cache_namespace,
            query,
            filters_fingerprint(filters)
        )
    }

    fn shard_name(&self, filters: &SearchFilters) -> String {
        if filters.agents.len() == 1 {
            filters
                .agents
                .iter()
                .next()
                .cloned()
                .unwrap_or_else(|| "global".into())
        } else {
            "global".into()
        }
    }

    fn cached_prefix_hits(&self, query: &str, filters: &SearchFilters) -> Option<Vec<CachedHit>> {
        if query.is_empty() {
            return None;
        }
        let cache = self.prefix_cache.lock().ok()?;
        let shard_name = self.shard_name(filters);
        let shard = cache.shard_opt(&shard_name)?;
        // Iterate over character boundaries to avoid slicing mid-codepoint.
        let mut byte_indices: Vec<usize> = query.char_indices().map(|(i, _)| i).collect();
        byte_indices.push(query.len());
        for &end in byte_indices.iter().rev() {
            if end == 0 {
                continue;
            }
            let key = self.cache_key(&query[..end], filters);
            if let Some(hits) = shard.peek(&key) {
                return Some(hits.clone());
            }
        }
        None
    }

    fn put_cache(&self, query: &str, filters: &SearchFilters, hits: &[SearchHit]) {
        if query.is_empty() || hits.is_empty() {
            return;
        }
        if let Ok(mut cache) = self.prefix_cache.lock() {
            let shard_name = self.shard_name(filters);
            let key = self.cache_key(query, filters);
            let cached_hits: Vec<CachedHit> = hits.iter().map(cached_hit_from).collect();
            cache.put(&shard_name, key, cached_hits);
        }
    }

    pub fn cache_stats(&self) -> CacheStats {
        let (hits, miss, shortfall, reloads, reload_ms_total) = self.metrics.snapshot_all();
        let (total_cap, total_cost, eviction_count, approx_bytes, byte_cap) =
            if let Ok(cache) = self.prefix_cache.lock() {
                (
                    cache.total_cap(),
                    cache.total_cost(),
                    cache.eviction_count(),
                    cache.total_bytes(),
                    cache.byte_cap(),
                )
            } else {
                (0, 0, 0, 0, 0)
            };
        CacheStats {
            cache_hits: hits,
            cache_miss: miss,
            cache_shortfall: shortfall,
            reloads,
            reload_ms_total,
            total_cap,
            total_cost,
            eviction_count,
            approx_bytes,
            byte_cap,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::connectors::{NormalizedConversation, NormalizedMessage, NormalizedSnippet};
    use crate::search::tantivy::TantivyIndex;
    use tempfile::TempDir;

    #[test]
    fn cache_prefix_lookup_handles_utf8_boundaries() {
        let client = SearchClient {
            reader: None,
            sqlite: None,
            prefix_cache: Mutex::new(CacheShards::new(*CACHE_TOTAL_CAP, *CACHE_BYTE_CAP)),
            last_reload: Mutex::new(None),
            last_generation: Mutex::new(None),
            reload_epoch: Arc::new(AtomicU64::new(0)),
            warm_tx: None,
            _warm_handle: None,
            _shared_filters: Arc::new(Mutex::new(())),
            metrics: Metrics::default(),
            cache_namespace: format!("v{}|schema:test", CACHE_KEY_VERSION),
        };

        let hits = vec![SearchHit {
            title: "こんにちは".into(),
            snippet: "".into(),
            content: "こんにちは 世界".into(),
            score: 1.0,
            source_path: "p".into(),
            agent: "a".into(),
            workspace: "w".into(),
            created_at: None,
            line_number: None,
            match_type: MatchType::Exact,
        }];

        client.put_cache("こん", &SearchFilters::default(), &hits);

        let cached = client
            .cached_prefix_hits("こんにちは", &SearchFilters::default())
            .unwrap();
        assert_eq!(cached.len(), 1);
        assert_eq!(cached[0].hit.title, "こんにちは");
    }

    #[test]
    fn bloom_gate_rejects_missing_terms() {
        let hit = SearchHit {
            title: "hello world".into(),
            snippet: "hello world".into(),
            content: "hello world".into(),
            score: 1.0,
            source_path: "p".into(),
            agent: "a".into(),
            workspace: "w".into(),
            created_at: None,
            line_number: None,
            match_type: MatchType::Exact,
        };
        let cached = cached_hit_from(&hit);
        assert!(hit_matches_query_cached(&cached, "hello"));
        assert!(!hit_matches_query_cached(&cached, "missing"));

        let metrics = Metrics::default();
        metrics.inc_cache_hits();
        metrics.inc_cache_miss();
        metrics.inc_cache_shortfall();
        metrics.inc_reload();
        assert_eq!(metrics.snapshot(), (1, 1, 1, 1));
    }

    #[test]
    fn search_returns_results_with_filters_and_pagination() -> Result<()> {
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;
        let conv = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("hello world convo".into()),
            workspace: Some(std::path::PathBuf::from("/tmp/workspace")),
            source_path: dir.path().join("rollout-1.jsonl"),
            started_at: Some(1_700_000_000_000),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: Some("me".into()),
                created_at: Some(1_700_000_000_000),
                content: "hello rust world".into(),
                extra: serde_json::json!({}),
                snippets: vec![NormalizedSnippet {
                    file_path: None,
                    start_line: None,
                    end_line: None,
                    language: None,
                    snippet_text: None,
                }],
            }],
        };
        index.add_conversation(&conv)?;
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");
        let mut filters = SearchFilters::default();
        filters.agents.insert("codex".into());

        let hits = client.search("hello", filters, 10, 0)?;
        assert_eq!(hits.len(), 1);
        assert_eq!(hits[0].agent, "codex");
        assert!(hits[0].snippet.contains("hello"));
        Ok(())
    }

    #[test]
    fn search_honors_created_range_and_workspace() -> Result<()> {
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;

        let conv_a = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("needle one".into()),
            workspace: Some(std::path::PathBuf::from("/ws/a")),
            source_path: dir.path().join("a.jsonl"),
            started_at: Some(10),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(10),
                content: "alpha needle".into(),
                extra: serde_json::json!({}),
                snippets: vec![NormalizedSnippet {
                    file_path: None,
                    start_line: None,
                    end_line: None,
                    language: None,
                    snippet_text: None,
                }],
            }],
        };
        let conv_b = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("needle two".into()),
            workspace: Some(std::path::PathBuf::from("/ws/b")),
            source_path: dir.path().join("b.jsonl"),
            started_at: Some(20),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(20),
                content: "\nneedle second line".into(),
                extra: serde_json::json!({}),
                snippets: vec![NormalizedSnippet {
                    file_path: None,
                    start_line: None,
                    end_line: None,
                    language: None,
                    snippet_text: None,
                }],
            }],
        };
        index.add_conversation(&conv_a)?;
        index.add_conversation(&conv_b)?;
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");
        let mut filters = SearchFilters::default();
        filters.workspaces.insert("/ws/b".into());
        filters.created_from = Some(15);
        filters.created_to = Some(25);

        let hits = client.search("needle", filters, 10, 0)?;
        assert_eq!(hits.len(), 1);
        assert_eq!(hits[0].workspace, "/ws/b");
        assert!(hits[0].snippet.contains("second line"));
        Ok(())
    }

    #[test]
    fn pagination_skips_results() -> Result<()> {
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;
        for i in 0..3 {
            let conv = NormalizedConversation {
                agent_slug: "codex".into(),
                external_id: None,
                title: Some(format!("doc-{i}")),
                workspace: Some(std::path::PathBuf::from("/ws/p")),
                source_path: dir.path().join(format!("{i}.jsonl")),
                started_at: Some(100 + i),
                ended_at: None,
                metadata: serde_json::json!({}),
                messages: vec![NormalizedMessage {
                    idx: 0,
                    role: "user".into(),
                    author: None,
                    created_at: Some(100 + i),
                    content: "pagination needle".into(),
                    extra: serde_json::json!({}),
                    snippets: vec![NormalizedSnippet {
                        file_path: None,
                        start_line: None,
                        end_line: None,
                        language: None,
                        snippet_text: None,
                    }],
                }],
            };
            index.add_conversation(&conv)?;
        }
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");
        let hits = client.search("pagination", SearchFilters::default(), 1, 1)?;
        assert_eq!(hits.len(), 1);
        Ok(())
    }

    #[test]
    fn search_matches_hyphenated_term() -> Result<()> {
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;
        let conv = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("cma-es notes".into()),
            workspace: Some(std::path::PathBuf::from("/tmp/workspace")),
            source_path: dir.path().join("rollout-1.jsonl"),
            started_at: Some(1_700_000_000_000),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: Some("me".into()),
                created_at: Some(1_700_000_000_000),
                content: "Need CMA-ES strategy and CMA ES variants".into(),
                extra: serde_json::json!({}),
                snippets: vec![NormalizedSnippet {
                    file_path: None,
                    start_line: None,
                    end_line: None,
                    language: None,
                    snippet_text: None,
                }],
            }],
        };
        index.add_conversation(&conv)?;
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");
        let hits = client.search("cma-es", SearchFilters::default(), 10, 0)?;
        assert_eq!(hits.len(), 1);
        assert!(hits[0].snippet.to_lowercase().contains("cma"));
        Ok(())
    }

    #[test]
    fn search_matches_prefix_edge_ngram() -> Result<()> {
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;
        let conv = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("math logic".into()),
            workspace: Some(std::path::PathBuf::from("/ws/m")),
            source_path: dir.path().join("math.jsonl"),
            started_at: Some(1000),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(1000),
                content: "please calculate the entropy".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        index.add_conversation(&conv)?;
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");

        // "cal" should match "calculate"
        let hits = client.search("cal", SearchFilters::default(), 10, 0)?;
        assert_eq!(hits.len(), 1);
        assert!(hits[0].content.contains("calculate"));

        // "entr" should match "entropy"
        let hits = client.search("entr", SearchFilters::default(), 10, 0)?;
        assert_eq!(hits.len(), 1);

        Ok(())
    }

    #[test]
    fn search_matches_snake_case() -> Result<()> {
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;
        let conv = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("code".into()),
            workspace: None,
            source_path: dir.path().join("c.jsonl"),
            started_at: Some(1),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(1),
                content: "check the my_variable_name please".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        index.add_conversation(&conv)?;
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");

        // "vari" should match "variable" inside "my_variable_name"
        let hits = client.search("vari", SearchFilters::default(), 10, 0)?;
        assert_eq!(hits.len(), 1);

        // "my_variable" should match "my_variable_name" (because it splits to "my variable")
        let hits = client.search("my_variable", SearchFilters::default(), 10, 0)?;
        assert_eq!(hits.len(), 1);

        Ok(())
    }

    #[test]
    fn search_matches_symbols_stripped() -> Result<()> {
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;
        let conv = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("symbols".into()),
            workspace: None,
            source_path: dir.path().join("s.jsonl"),
            started_at: Some(1),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(1),
                content: "working with c++ and foo.bar today".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        index.add_conversation(&conv)?;
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");

        // "c++" -> "c"
        let hits = client.search("c++", SearchFilters::default(), 10, 0)?;
        assert_eq!(hits.len(), 1);

        // "foo.bar" -> "foo", "bar"
        let hits = client.search("foo.bar", SearchFilters::default(), 10, 0)?;
        assert_eq!(hits.len(), 1);

        Ok(())
    }

    #[test]
    fn search_sets_match_type_for_wildcards() -> Result<()> {
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;

        let conv = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("handlers".into()),
            workspace: None,
            source_path: dir.path().join("h.jsonl"),
            started_at: Some(1),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(1),
                content: "the request handler delegates".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        index.add_conversation(&conv)?;
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");

        let exact = client.search("handler", SearchFilters::default(), 10, 0)?;
        assert_eq!(exact[0].match_type, MatchType::Exact);

        let prefix = client.search("hand*", SearchFilters::default(), 10, 0)?;
        assert_eq!(prefix[0].match_type, MatchType::Prefix);

        let suffix = client.search("*handler", SearchFilters::default(), 10, 0)?;
        assert_eq!(suffix[0].match_type, MatchType::Suffix);

        let substring = client.search("*andle*", SearchFilters::default(), 10, 0)?;
        assert_eq!(substring[0].match_type, MatchType::Substring);

        Ok(())
    }

    #[test]
    fn search_with_fallback_marks_implicit_wildcard() -> Result<()> {
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;

        let conv = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("handlers".into()),
            workspace: None,
            source_path: dir.path().join("h2.jsonl"),
            started_at: Some(1),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(1),
                content: "the request handler delegates".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        index.add_conversation(&conv)?;
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");

        // Base search for "andle" finds nothing; fallback "*andle*" should hit and mark implicit.
        let result = client.search_with_fallback("andle", SearchFilters::default(), 10, 0, 2)?;
        assert!(result.wildcard_fallback);
        assert_eq!(result.hits.len(), 1);
        assert_eq!(result.hits[0].match_type, MatchType::ImplicitWildcard);

        Ok(())
    }

    #[test]
    fn sqlite_backend_skips_wildcard_queries() -> Result<()> {
        // Build a client with SQLite only; wildcard queries should short-circuit without errors.
        let conn = Connection::open_in_memory()?;
        let client = SearchClient {
            reader: None,
            sqlite: Some(conn),
            prefix_cache: Mutex::new(CacheShards::new(*CACHE_TOTAL_CAP, *CACHE_BYTE_CAP)),
            last_reload: Mutex::new(None),
            last_generation: Mutex::new(None),
            reload_epoch: Arc::new(AtomicU64::new(0)),
            warm_tx: None,
            _warm_handle: None,
            _shared_filters: Arc::new(Mutex::new(())),
            metrics: Metrics::default(),
            cache_namespace: format!("v{}|schema:test", CACHE_KEY_VERSION),
        };

        let hits = client.search("*handler", SearchFilters::default(), 5, 0)?;
        assert!(
            hits.is_empty(),
            "wildcard should skip sqlite fallback, not error"
        );

        Ok(())
    }

    #[test]
    fn cache_invalidates_on_new_data() -> Result<()> {
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;

        // 1. Add initial doc
        let conv1 = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("first".into()),
            workspace: None,
            source_path: dir.path().join("1.jsonl"),
            started_at: Some(1),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(1),
                content: "apple banana".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        index.add_conversation(&conv1)?;
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");

        // 2. Search "app" -> should hit "apple"
        let hits = client.search("app", SearchFilters::default(), 10, 0)?;
        assert_eq!(hits.len(), 1);
        assert_eq!(hits[0].content, "apple banana");

        // 3. Verify it's cached (peek internal state)
        {
            let cache = client.prefix_cache.lock().unwrap();
            let shard = cache.shard_opt("global").unwrap();
            // "app" should be in cache
            assert!(shard.contains(&client.cache_key("app", &SearchFilters::default())));
        }

        // 4. Add new doc with "apricot"
        let conv2 = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("second".into()),
            workspace: None,
            source_path: dir.path().join("2.jsonl"),
            started_at: Some(2),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(2),
                content: "apricot".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        index.add_conversation(&conv2)?;
        index.commit()?;

        // 5. Force reload (mocking time passing or just ensuring reload triggers)
        // In test, maybe_reload_reader uses 300ms debounce.
        // We can rely on opstamp check logic which runs AFTER reload.
        // We need to sleep briefly to bypass debounce or just modify test to not rely on time?
        // Actually SearchClient::maybe_reload_reader checks duration.
        std::thread::sleep(std::time::Duration::from_millis(350));

        // 6. Search "ap" (prefix of apricot and apple)
        // The cache for "app" should be cleared if opstamp changed.
        let _hits = client.search("app", SearchFilters::default(), 10, 0)?;
        // Should now find 1 doc still ("apple"), but cache should have been cleared first

        // Search "apr" -> should find "apricot"
        let hits = client.search("apr", SearchFilters::default(), 10, 0)?;
        assert_eq!(hits.len(), 1);
        assert_eq!(hits[0].content, "apricot");

        // Check that cache was cleared by verifying a stale key is gone?
        // Or rely on correctness of results if we searched a common prefix?

        Ok(())
    }

    #[test]
    fn track_generation_clears_cache_on_change() {
        let client = SearchClient {
            reader: None,
            sqlite: None,
            prefix_cache: Mutex::new(CacheShards::new(*CACHE_TOTAL_CAP, *CACHE_BYTE_CAP)),
            last_reload: Mutex::new(None),
            last_generation: Mutex::new(None),
            reload_epoch: Arc::new(AtomicU64::new(0)),
            warm_tx: None,
            _warm_handle: None,
            _shared_filters: Arc::new(Mutex::new(())),
            metrics: Metrics::default(),
            cache_namespace: format!("v{}|schema:test", CACHE_KEY_VERSION),
        };

        let hit = SearchHit {
            title: "hello world".into(),
            snippet: "hello".into(),
            content: "hello world".into(),
            score: 1.0,
            source_path: "p".into(),
            agent: "a".into(),
            workspace: "w".into(),
            created_at: None,
            line_number: None,
            match_type: MatchType::Exact,
        };
        let hits = vec![hit];

        client.put_cache("hello", &SearchFilters::default(), &hits);
        {
            let cache = client.prefix_cache.lock().unwrap();
            assert!(!cache.shards.is_empty());
        }

        client.track_generation(1);
        {
            let cache = client.prefix_cache.lock().unwrap();
            assert!(!cache.shards.is_empty());
        }

        client.track_generation(2);
        {
            let cache = client.prefix_cache.lock().unwrap();
            assert!(cache.shards.is_empty());
        }
    }

    #[test]
    fn cache_total_cap_evicts_across_shards() {
        let client = SearchClient {
            reader: None,
            sqlite: None,
            prefix_cache: Mutex::new(CacheShards::new(2, 0)), // tiny entry cap, no byte cap
            last_reload: Mutex::new(None),
            last_generation: Mutex::new(None),
            reload_epoch: Arc::new(AtomicU64::new(0)),
            warm_tx: None,
            _warm_handle: None,
            _shared_filters: Arc::new(Mutex::new(())),
            metrics: Metrics::default(),
            cache_namespace: format!("v{}|schema:test", CACHE_KEY_VERSION),
        };

        let hit = SearchHit {
            title: "a".into(),
            snippet: "a".into(),
            content: "a".into(),
            score: 1.0,
            source_path: "p".into(),
            agent: "agent1".into(),
            workspace: "w".into(),
            created_at: None,
            line_number: None,
            match_type: MatchType::Exact,
        };
        let hits = vec![hit.clone()];

        let mut filters = SearchFilters::default();
        filters.agents.insert("agent1".into());
        client.put_cache("a", &filters, &hits);
        filters.agents.clear();
        filters.agents.insert("agent2".into());
        client.put_cache("b", &filters, &hits);
        filters.agents.clear();
        filters.agents.insert("agent3".into());
        client.put_cache("c", &filters, &hits);

        let stats = client.cache_stats();
        assert!(stats.total_cost <= stats.total_cap);
        assert_eq!(stats.total_cap, 2);
    }

    #[test]
    fn cache_stats_reflect_metrics() {
        let client = SearchClient {
            reader: None,
            sqlite: None,
            prefix_cache: Mutex::new(CacheShards::new(*CACHE_TOTAL_CAP, *CACHE_BYTE_CAP)),
            last_reload: Mutex::new(None),
            last_generation: Mutex::new(None),
            reload_epoch: Arc::new(AtomicU64::new(0)),
            warm_tx: None,
            _warm_handle: None,
            _shared_filters: Arc::new(Mutex::new(())),
            metrics: Metrics::default(),
            cache_namespace: format!("v{}|schema:test", CACHE_KEY_VERSION),
        };

        client.metrics.inc_cache_hits();
        client.metrics.inc_cache_miss();
        client.metrics.inc_cache_shortfall();
        client.metrics.record_reload(Duration::from_millis(10));

        let stats = client.cache_stats();
        assert_eq!(stats.cache_hits, 1);
        assert_eq!(stats.cache_miss, 1);
        assert_eq!(stats.cache_shortfall, 1);
        assert_eq!(stats.reloads, 1);
        assert_eq!(stats.reload_ms_total, 10);
        assert_eq!(stats.total_cap, *CACHE_TOTAL_CAP);
    }

    #[test]
    fn cache_eviction_count_tracks_evictions() {
        // tiny entry cap (2 entries), no byte cap - forces evictions
        let client = SearchClient {
            reader: None,
            sqlite: None,
            prefix_cache: Mutex::new(CacheShards::new(2, 0)),
            last_reload: Mutex::new(None),
            last_generation: Mutex::new(None),
            reload_epoch: Arc::new(AtomicU64::new(0)),
            warm_tx: None,
            _warm_handle: None,
            _shared_filters: Arc::new(Mutex::new(())),
            metrics: Metrics::default(),
            cache_namespace: format!("v{}|schema:test", CACHE_KEY_VERSION),
        };

        let hit = SearchHit {
            title: "test".into(),
            snippet: "snippet".into(),
            content: "content".into(),
            score: 1.0,
            source_path: "p".into(),
            agent: "a".into(),
            workspace: "w".into(),
            created_at: None,
            line_number: None,
            match_type: MatchType::Exact,
        };

        // Put 3 entries - should trigger 1 eviction (cap is 2)
        client.put_cache(
            "query1",
            &SearchFilters::default(),
            std::slice::from_ref(&hit),
        );
        client.put_cache(
            "query2",
            &SearchFilters::default(),
            std::slice::from_ref(&hit),
        );
        client.put_cache(
            "query3",
            &SearchFilters::default(),
            std::slice::from_ref(&hit),
        );

        let stats = client.cache_stats();
        assert!(
            stats.eviction_count >= 1,
            "should have evicted at least 1 entry"
        );
        assert!(stats.total_cost <= 2, "should be at or below cap");
        assert!(stats.approx_bytes > 0, "should track bytes used");
    }

    #[test]
    fn cache_byte_cap_triggers_eviction() {
        // Large entry cap (1000), tiny byte cap (100 bytes) - forces byte-based evictions
        let client = SearchClient {
            reader: None,
            sqlite: None,
            prefix_cache: Mutex::new(CacheShards::new(1000, 100)), // byte cap of 100
            last_reload: Mutex::new(None),
            last_generation: Mutex::new(None),
            reload_epoch: Arc::new(AtomicU64::new(0)),
            warm_tx: None,
            _warm_handle: None,
            _shared_filters: Arc::new(Mutex::new(())),
            metrics: Metrics::default(),
            cache_namespace: format!("v{}|schema:test", CACHE_KEY_VERSION),
        };

        // Large content to exceed byte cap quickly
        let hit = SearchHit {
            title: "a".repeat(50),
            snippet: "b".repeat(50),
            content: "c".repeat(100), // 200+ bytes per hit
            score: 1.0,
            source_path: "p".into(),
            agent: "a".into(),
            workspace: "w".into(),
            created_at: None,
            line_number: None,
            match_type: MatchType::Exact,
        };

        // Put 3 large entries - should trigger byte-based evictions
        client.put_cache("q1", &SearchFilters::default(), std::slice::from_ref(&hit));
        client.put_cache("q2", &SearchFilters::default(), std::slice::from_ref(&hit));
        client.put_cache("q3", &SearchFilters::default(), std::slice::from_ref(&hit));

        let stats = client.cache_stats();
        assert!(
            stats.eviction_count >= 1,
            "byte cap should trigger evictions"
        );
        assert_eq!(stats.byte_cap, 100, "byte cap should be reported");
        // Note: approx_bytes may briefly exceed cap during put, but eviction brings it down
    }

    // ============================================================
    // Phase 7 Tests: WildcardPattern, escape_regex, fallback, dedup
    // ============================================================

    #[test]
    fn wildcard_pattern_parse_exact() {
        // No wildcards - exact match
        assert_eq!(
            WildcardPattern::parse("hello"),
            WildcardPattern::Exact("hello".into())
        );
        assert_eq!(
            WildcardPattern::parse("HELLO"),
            WildcardPattern::Exact("hello".into()) // lowercased
        );
        assert_eq!(
            WildcardPattern::parse("FooBar123"),
            WildcardPattern::Exact("foobar123".into())
        );
    }

    #[test]
    fn wildcard_pattern_parse_prefix() {
        // Trailing wildcard: foo*
        assert_eq!(
            WildcardPattern::parse("foo*"),
            WildcardPattern::Prefix("foo".into())
        );
        assert_eq!(
            WildcardPattern::parse("CONFIG*"),
            WildcardPattern::Prefix("config".into())
        );
        assert_eq!(
            WildcardPattern::parse("test*"),
            WildcardPattern::Prefix("test".into())
        );
    }

    #[test]
    fn wildcard_pattern_parse_suffix() {
        // Leading wildcard: *foo
        assert_eq!(
            WildcardPattern::parse("*foo"),
            WildcardPattern::Suffix("foo".into())
        );
        assert_eq!(
            WildcardPattern::parse("*Error"),
            WildcardPattern::Suffix("error".into())
        );
        assert_eq!(
            WildcardPattern::parse("*Handler"),
            WildcardPattern::Suffix("handler".into())
        );
    }

    #[test]
    fn wildcard_pattern_parse_substring() {
        // Both wildcards: *foo*
        assert_eq!(
            WildcardPattern::parse("*foo*"),
            WildcardPattern::Substring("foo".into())
        );
        assert_eq!(
            WildcardPattern::parse("*CONFIG*"),
            WildcardPattern::Substring("config".into())
        );
        assert_eq!(
            WildcardPattern::parse("*test*"),
            WildcardPattern::Substring("test".into())
        );
    }

    #[test]
    fn wildcard_pattern_parse_edge_cases() {
        // Empty after trimming wildcards
        assert_eq!(
            WildcardPattern::parse("*"),
            WildcardPattern::Exact("".into())
        );
        assert_eq!(
            WildcardPattern::parse("**"),
            WildcardPattern::Exact("".into())
        );
        assert_eq!(
            WildcardPattern::parse("***"),
            WildcardPattern::Exact("".into())
        );

        // Single char with wildcards
        assert_eq!(
            WildcardPattern::parse("*a*"),
            WildcardPattern::Substring("a".into())
        );
        assert_eq!(
            WildcardPattern::parse("a*"),
            WildcardPattern::Prefix("a".into())
        );
        assert_eq!(
            WildcardPattern::parse("*a"),
            WildcardPattern::Suffix("a".into())
        );

        // Multiple asterisks get trimmed
        assert_eq!(
            WildcardPattern::parse("***foo***"),
            WildcardPattern::Substring("foo".into())
        );
    }

    #[test]
    fn wildcard_pattern_to_regex_suffix() {
        let pattern = WildcardPattern::Suffix("foo".into());
        assert_eq!(pattern.to_regex(), Some(".*foo".into()));
    }

    #[test]
    fn wildcard_pattern_to_regex_substring() {
        let pattern = WildcardPattern::Substring("bar".into());
        assert_eq!(pattern.to_regex(), Some(".*bar.*".into()));
    }

    #[test]
    fn wildcard_pattern_to_regex_exact_prefix_none() {
        // Exact and Prefix patterns don't need regex
        let exact = WildcardPattern::Exact("foo".into());
        assert_eq!(exact.to_regex(), None);

        let prefix = WildcardPattern::Prefix("bar".into());
        assert_eq!(prefix.to_regex(), None);
    }

    #[test]
    fn match_type_quality_factors() {
        // Exact match has highest quality
        assert_eq!(MatchType::Exact.quality_factor(), 1.0);
        // Prefix is slightly lower
        assert_eq!(MatchType::Prefix.quality_factor(), 0.9);
        // Suffix is lower than prefix
        assert_eq!(MatchType::Suffix.quality_factor(), 0.8);
        // Substring is lower still
        assert_eq!(MatchType::Substring.quality_factor(), 0.7);
        // Implicit wildcard is lowest
        assert_eq!(MatchType::ImplicitWildcard.quality_factor(), 0.6);
    }

    #[test]
    fn wildcard_pattern_to_match_type() {
        assert_eq!(
            WildcardPattern::Exact("foo".into()).to_match_type(),
            MatchType::Exact
        );
        assert_eq!(
            WildcardPattern::Prefix("foo".into()).to_match_type(),
            MatchType::Prefix
        );
        assert_eq!(
            WildcardPattern::Suffix("foo".into()).to_match_type(),
            MatchType::Suffix
        );
        assert_eq!(
            WildcardPattern::Substring("foo".into()).to_match_type(),
            MatchType::Substring
        );
    }

    #[test]
    fn dominant_match_type_single_terms() {
        // Single terms return their pattern's match type
        assert_eq!(dominant_match_type("hello"), MatchType::Exact);
        assert_eq!(dominant_match_type("hello*"), MatchType::Prefix);
        assert_eq!(dominant_match_type("*hello"), MatchType::Suffix);
        assert_eq!(dominant_match_type("*hello*"), MatchType::Substring);
    }

    #[test]
    fn dominant_match_type_multiple_terms() {
        // Multiple terms: returns the "loosest" (lowest quality factor)
        assert_eq!(dominant_match_type("foo bar"), MatchType::Exact);
        assert_eq!(dominant_match_type("foo bar*"), MatchType::Prefix);
        assert_eq!(dominant_match_type("foo *bar"), MatchType::Suffix);
        assert_eq!(dominant_match_type("foo* *bar*"), MatchType::Substring);
        // Substring is loosest even if other terms are exact
        assert_eq!(dominant_match_type("foo *bar* baz"), MatchType::Substring);
    }

    #[test]
    fn dominant_match_type_empty_query() {
        assert_eq!(dominant_match_type(""), MatchType::Exact);
        assert_eq!(dominant_match_type("   "), MatchType::Exact);
    }

    #[test]
    fn escape_regex_basic() {
        // Plain text should pass through unchanged
        assert_eq!(escape_regex("hello"), "hello");
        assert_eq!(escape_regex("foo123"), "foo123");
        assert_eq!(escape_regex(""), "");
    }

    #[test]
    fn escape_regex_special_chars() {
        // All special regex chars should be escaped
        assert_eq!(escape_regex("."), "\\.");
        assert_eq!(escape_regex("*"), "\\*");
        assert_eq!(escape_regex("+"), "\\+");
        assert_eq!(escape_regex("?"), "\\?");
        assert_eq!(escape_regex("("), "\\(");
        assert_eq!(escape_regex(")"), "\\)");
        assert_eq!(escape_regex("["), "\\[");
        assert_eq!(escape_regex("]"), "\\]");
        assert_eq!(escape_regex("{"), "\\{");
        assert_eq!(escape_regex("}"), "\\}");
        assert_eq!(escape_regex("|"), "\\|");
        assert_eq!(escape_regex("^"), "\\^");
        assert_eq!(escape_regex("$"), "\\$");
        assert_eq!(escape_regex("\\"), "\\\\");
    }

    #[test]
    fn escape_regex_complex_patterns() {
        // Complex patterns with multiple special chars
        assert_eq!(escape_regex("foo.bar"), "foo\\.bar");
        assert_eq!(escape_regex("test[0-9]+"), "test\\[0-9\\]\\+");
        assert_eq!(escape_regex("(a|b)"), "\\(a\\|b\\)");
        assert_eq!(escape_regex("end$"), "end\\$");
        assert_eq!(escape_regex("^start"), "\\^start");
        assert_eq!(escape_regex("a*b+c?"), "a\\*b\\+c\\?");
    }

    #[test]
    fn is_tool_invocation_noise_detects_noise() {
        // Short tool invocations are noise
        assert!(is_tool_invocation_noise("[Tool: Bash - Check status]"));
        assert!(is_tool_invocation_noise("[Tool: Read]"));
        assert!(is_tool_invocation_noise("  [Tool: Grep - Search files]  "));

        // Very short tool markers
        assert!(is_tool_invocation_noise("[tool]"));
        assert!(is_tool_invocation_noise("tool: Bash"));
    }

    #[test]
    fn is_tool_invocation_noise_allows_content() {
        // Real content should not be flagged
        assert!(!is_tool_invocation_noise(
            "This is a normal message about tools"
        ));
        assert!(!is_tool_invocation_noise("The tool worked correctly"));
        assert!(!is_tool_invocation_noise(
            "I'll use the Read tool to check the file contents and then analyze the code structure for potential improvements."
        ));
    }

    #[test]
    fn deduplicate_hits_removes_exact_dupes() {
        let hits = vec![
            SearchHit {
                title: "title1".into(),
                snippet: "snip1".into(),
                content: "hello world".into(),
                score: 1.0,
                source_path: "a.jsonl".into(),
                agent: "agent".into(),
                workspace: "ws".into(),
                created_at: Some(100),
                line_number: None,
                match_type: MatchType::Exact,
            },
            SearchHit {
                title: "title2".into(),
                snippet: "snip2".into(),
                content: "hello world".into(), // same content
                score: 0.5,                    // lower score
                source_path: "b.jsonl".into(),
                agent: "agent".into(),
                workspace: "ws".into(),
                created_at: Some(200),
                line_number: None,
                match_type: MatchType::Exact,
            },
        ];

        let deduped = deduplicate_hits(hits);
        assert_eq!(deduped.len(), 1);
        assert_eq!(deduped[0].score, 1.0); // kept higher score
        assert_eq!(deduped[0].title, "title1");
    }

    #[test]
    fn deduplicate_hits_keeps_higher_score() {
        let hits = vec![
            SearchHit {
                title: "title1".into(),
                snippet: "snip1".into(),
                content: "hello world".into(),
                score: 0.3, // lower score first
                source_path: "a.jsonl".into(),
                agent: "agent".into(),
                workspace: "ws".into(),
                created_at: Some(100),
                line_number: None,
                match_type: MatchType::Exact,
            },
            SearchHit {
                title: "title2".into(),
                snippet: "snip2".into(),
                content: "hello world".into(),
                score: 0.9, // higher score second
                source_path: "b.jsonl".into(),
                agent: "agent".into(),
                workspace: "ws".into(),
                created_at: Some(200),
                line_number: None,
                match_type: MatchType::Exact,
            },
        ];

        let deduped = deduplicate_hits(hits);
        assert_eq!(deduped.len(), 1);
        assert_eq!(deduped[0].score, 0.9); // kept higher score
        assert_eq!(deduped[0].title, "title2");
    }

    #[test]
    fn deduplicate_hits_normalizes_whitespace() {
        let hits = vec![
            SearchHit {
                title: "title1".into(),
                snippet: "snip1".into(),
                content: "hello    world".into(), // extra spaces
                score: 1.0,
                source_path: "a.jsonl".into(),
                agent: "agent".into(),
                workspace: "ws".into(),
                created_at: Some(100),
                line_number: None,
                match_type: MatchType::Exact,
            },
            SearchHit {
                title: "title2".into(),
                snippet: "snip2".into(),
                content: "hello world".into(), // normal spacing
                score: 0.5,
                source_path: "b.jsonl".into(),
                agent: "agent".into(),
                workspace: "ws".into(),
                created_at: Some(200),
                line_number: None,
                match_type: MatchType::Exact,
            },
        ];

        let deduped = deduplicate_hits(hits);
        assert_eq!(deduped.len(), 1); // normalized to same content
    }

    #[test]
    fn deduplicate_hits_filters_tool_noise() {
        let hits = vec![
            SearchHit {
                title: "title1".into(),
                snippet: "snip1".into(),
                content: "[Tool: Bash - Run tests]".into(), // noise
                score: 1.0,
                source_path: "a.jsonl".into(),
                agent: "agent".into(),
                workspace: "ws".into(),
                created_at: Some(100),
                line_number: None,
                match_type: MatchType::Exact,
            },
            SearchHit {
                title: "title2".into(),
                snippet: "snip2".into(),
                content: "This is real content about testing".into(),
                score: 0.5,
                source_path: "b.jsonl".into(),
                agent: "agent".into(),
                workspace: "ws".into(),
                created_at: Some(200),
                line_number: None,
                match_type: MatchType::Exact,
            },
        ];

        let deduped = deduplicate_hits(hits);
        assert_eq!(deduped.len(), 1);
        assert!(deduped[0].content.contains("real content"));
    }

    #[test]
    fn deduplicate_hits_preserves_unique_content() {
        let hits = vec![
            SearchHit {
                title: "title1".into(),
                snippet: "snip1".into(),
                content: "first message".into(),
                score: 1.0,
                source_path: "a.jsonl".into(),
                agent: "agent".into(),
                workspace: "ws".into(),
                created_at: Some(100),
                line_number: None,
                match_type: MatchType::Exact,
            },
            SearchHit {
                title: "title2".into(),
                snippet: "snip2".into(),
                content: "second message".into(),
                score: 0.8,
                source_path: "b.jsonl".into(),
                agent: "agent".into(),
                workspace: "ws".into(),
                created_at: Some(200),
                line_number: None,
                match_type: MatchType::Exact,
            },
            SearchHit {
                title: "title3".into(),
                snippet: "snip3".into(),
                content: "third message".into(),
                score: 0.6,
                source_path: "c.jsonl".into(),
                agent: "agent".into(),
                workspace: "ws".into(),
                created_at: Some(300),
                line_number: None,
                match_type: MatchType::Exact,
            },
        ];

        let deduped = deduplicate_hits(hits);
        assert_eq!(deduped.len(), 3); // all unique
    }

    #[test]
    fn search_with_fallback_returns_exact_when_sufficient() -> Result<()> {
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;

        // Add enough docs to exceed threshold - each with UNIQUE content to avoid dedup
        for i in 0..5 {
            let conv = NormalizedConversation {
                agent_slug: "codex".into(),
                external_id: None,
                title: Some(format!("doc-{i}")),
                workspace: Some(std::path::PathBuf::from("/ws")),
                source_path: dir.path().join(format!("{i}.jsonl")),
                started_at: Some(100 + i),
                ended_at: None,
                metadata: serde_json::json!({}),
                messages: vec![NormalizedMessage {
                    idx: 0,
                    role: "user".into(),
                    author: None,
                    created_at: Some(100 + i),
                    // Each doc has unique content but shares "apple" keyword
                    content: format!("apple fruit number {i} is delicious and healthy"),
                    extra: serde_json::json!({}),
                    snippets: vec![],
                }],
            };
            index.add_conversation(&conv)?;
        }
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");

        // Search with low threshold - should not trigger fallback
        let result = client.search_with_fallback(
            "apple",
            SearchFilters::default(),
            10,
            0,
            3, // threshold of 3
        )?;

        assert!(!result.wildcard_fallback);
        assert!(result.hits.len() >= 3); // has enough results

        Ok(())
    }

    #[test]
    fn search_with_fallback_triggers_on_sparse_results() -> Result<()> {
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;

        // Add docs with substring that won't match exact prefix
        let conv = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("substring test".into()),
            workspace: Some(std::path::PathBuf::from("/ws")),
            source_path: dir.path().join("test.jsonl"),
            started_at: Some(100),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(100),
                content: "configuration management system".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        index.add_conversation(&conv)?;
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");

        // Search for "config" which should match "configuration" via prefix
        let result = client.search_with_fallback(
            "config",
            SearchFilters::default(),
            10,
            0,
            5, // high threshold
        )?;

        // Since we have only 1 result and threshold is 5, it may trigger fallback
        // but *config* would still match "configuration"
        assert!(!result.hits.is_empty());

        Ok(())
    }

    #[test]
    fn search_with_fallback_skips_when_query_has_wildcards() -> Result<()> {
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;

        let conv = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("test".into()),
            workspace: None,
            source_path: dir.path().join("test.jsonl"),
            started_at: Some(100),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(100),
                content: "testing data".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        index.add_conversation(&conv)?;
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");

        // Query already has wildcards - should not trigger fallback
        let result = client.search_with_fallback(
            "*test*",
            SearchFilters::default(),
            10,
            0,
            10, // high threshold
        )?;

        assert!(!result.wildcard_fallback); // shouldn't trigger fallback for wildcard queries
        Ok(())
    }

    #[test]
    fn search_with_fallback_skips_empty_query() -> Result<()> {
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;

        let conv = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("test".into()),
            workspace: None,
            source_path: dir.path().join("test.jsonl"),
            started_at: Some(100),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(100),
                content: "testing data".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        index.add_conversation(&conv)?;
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");

        // Empty query - should not trigger fallback
        let result = client.search_with_fallback("  ", SearchFilters::default(), 10, 0, 10)?;

        assert!(!result.wildcard_fallback);
        Ok(())
    }

    #[test]
    fn sanitize_query_preserves_wildcards() {
        // Wildcards should be preserved
        assert_eq!(sanitize_query("*foo*"), "*foo*");
        assert_eq!(sanitize_query("foo*"), "foo*");
        assert_eq!(sanitize_query("*bar"), "*bar");
        assert_eq!(sanitize_query("*config*"), "*config*");
    }

    #[test]
    fn sanitize_query_strips_other_special_chars() {
        // Non-wildcard special chars become spaces
        assert_eq!(sanitize_query("foo.bar"), "foo bar");
        assert_eq!(sanitize_query("c++"), "c  ");
        assert_eq!(sanitize_query("foo-bar"), "foo bar");
        assert_eq!(sanitize_query("test_case"), "test case");
    }

    #[test]
    fn sanitize_query_combined() {
        // Mix of wildcards and special chars
        assert_eq!(sanitize_query("*foo.bar*"), "*foo bar*");
        assert_eq!(sanitize_query("test-*"), "test *");
        assert_eq!(sanitize_query("*c++*"), "*c  *");
    }

    // Boolean query parsing tests
    #[test]
    fn parse_boolean_query_simple_terms() {
        let tokens = parse_boolean_query("foo bar baz");
        assert_eq!(tokens.len(), 3);
        assert_eq!(tokens[0], QueryToken::Term("foo".to_string()));
        assert_eq!(tokens[1], QueryToken::Term("bar".to_string()));
        assert_eq!(tokens[2], QueryToken::Term("baz".to_string()));
    }

    #[test]
    fn parse_boolean_query_and_operator() {
        let tokens = parse_boolean_query("foo AND bar");
        assert_eq!(tokens.len(), 3);
        assert_eq!(tokens[0], QueryToken::Term("foo".to_string()));
        assert_eq!(tokens[1], QueryToken::And);
        assert_eq!(tokens[2], QueryToken::Term("bar".to_string()));

        // Also test && syntax
        let tokens2 = parse_boolean_query("foo && bar");
        assert_eq!(tokens2.len(), 3);
        assert_eq!(tokens2[1], QueryToken::And);
    }

    #[test]
    fn parse_boolean_query_or_operator() {
        let tokens = parse_boolean_query("foo OR bar");
        assert_eq!(tokens.len(), 3);
        assert_eq!(tokens[0], QueryToken::Term("foo".to_string()));
        assert_eq!(tokens[1], QueryToken::Or);
        assert_eq!(tokens[2], QueryToken::Term("bar".to_string()));

        // Also test || syntax
        let tokens2 = parse_boolean_query("foo || bar");
        assert_eq!(tokens2.len(), 3);
        assert_eq!(tokens2[1], QueryToken::Or);
    }

    #[test]
    fn parse_boolean_query_not_operator() {
        let tokens = parse_boolean_query("foo NOT bar");
        assert_eq!(tokens.len(), 3);
        assert_eq!(tokens[0], QueryToken::Term("foo".to_string()));
        assert_eq!(tokens[1], QueryToken::Not);
        assert_eq!(tokens[2], QueryToken::Term("bar".to_string()));
    }

    #[test]
    fn parse_boolean_query_quoted_phrase() {
        let tokens = parse_boolean_query(r#"foo "exact phrase" bar"#);
        assert_eq!(tokens.len(), 3);
        assert_eq!(tokens[0], QueryToken::Term("foo".to_string()));
        assert_eq!(tokens[1], QueryToken::Phrase("exact phrase".to_string()));
        assert_eq!(tokens[2], QueryToken::Term("bar".to_string()));
    }

    #[test]
    fn parse_boolean_query_complex() {
        let tokens = parse_boolean_query(r#"error OR warning NOT "false positive""#);
        assert_eq!(tokens.len(), 5);
        assert_eq!(tokens[0], QueryToken::Term("error".to_string()));
        assert_eq!(tokens[1], QueryToken::Or);
        assert_eq!(tokens[2], QueryToken::Term("warning".to_string()));
        assert_eq!(tokens[3], QueryToken::Not);
        assert_eq!(tokens[4], QueryToken::Phrase("false positive".to_string()));
    }

    #[test]
    fn has_boolean_operators_detection() {
        assert!(!has_boolean_operators("foo bar"));
        assert!(has_boolean_operators("foo AND bar"));
        assert!(has_boolean_operators("foo OR bar"));
        assert!(has_boolean_operators("foo NOT bar"));
        assert!(has_boolean_operators(r#""exact phrase""#));
        assert!(has_boolean_operators("foo && bar"));
        assert!(has_boolean_operators("foo || bar"));
    }

    #[test]
    fn parse_boolean_query_case_insensitive_operators() {
        // Operators should be case-insensitive
        let tokens = parse_boolean_query("foo and bar or baz not qux");
        assert_eq!(tokens.len(), 7);
        assert_eq!(tokens[1], QueryToken::And);
        assert_eq!(tokens[3], QueryToken::Or);
        assert_eq!(tokens[5], QueryToken::Not);
    }

    #[test]
    fn parse_boolean_query_with_wildcards() {
        let tokens = parse_boolean_query("*config* OR env*");
        assert_eq!(tokens.len(), 3);
        assert_eq!(tokens[0], QueryToken::Term("*config*".to_string()));
        assert_eq!(tokens[1], QueryToken::Or);
        assert_eq!(tokens[2], QueryToken::Term("env*".to_string()));
    }

    // ============================================================
    // Filter Fidelity Property Tests (glt.9)
    // Verify filters are never violated in search results
    // ============================================================

    #[test]
    fn filter_fidelity_agent_filter_respected() -> Result<()> {
        // Multiple agents; filter should return only matching agent
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;

        // Agent A (codex)
        let conv_a = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("alpha doc".into()),
            workspace: None,
            source_path: dir.path().join("a.jsonl"),
            started_at: Some(100),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(100),
                content: "hello world findme alpha".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        // Agent B (claude)
        let conv_b = NormalizedConversation {
            agent_slug: "claude".into(),
            external_id: None,
            title: Some("beta doc".into()),
            workspace: None,
            source_path: dir.path().join("b.jsonl"),
            started_at: Some(200),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(200),
                content: "hello world findme beta".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        index.add_conversation(&conv_a)?;
        index.add_conversation(&conv_b)?;
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");

        // Search with agent filter for codex only
        let mut filters = SearchFilters::default();
        filters.agents.insert("codex".into());

        let hits = client.search("findme", filters.clone(), 10, 0)?;

        // Property: all results must have agent == "codex"
        for hit in &hits {
            assert_eq!(
                hit.agent, "codex",
                "Agent filter violated: got agent '{}' instead of 'codex'",
                hit.agent
            );
        }
        assert!(!hits.is_empty(), "Should have found results");

        // Repeat search (should use cache) and verify same property
        let cached_hits = client.search("findme", filters, 10, 0)?;
        for hit in &cached_hits {
            assert_eq!(hit.agent, "codex", "Cached search violated agent filter");
        }

        Ok(())
    }

    #[test]
    fn filter_fidelity_workspace_filter_respected() -> Result<()> {
        // Multiple workspaces; filter should return only matching workspace
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;

        // Workspace A
        let conv_a = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("ws_a doc".into()),
            workspace: Some(std::path::PathBuf::from("/workspace/alpha")),
            source_path: dir.path().join("a.jsonl"),
            started_at: Some(100),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(100),
                content: "workspace test needle".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        // Workspace B
        let conv_b = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("ws_b doc".into()),
            workspace: Some(std::path::PathBuf::from("/workspace/beta")),
            source_path: dir.path().join("b.jsonl"),
            started_at: Some(200),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(200),
                content: "workspace test needle".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        index.add_conversation(&conv_a)?;
        index.add_conversation(&conv_b)?;
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");

        // Search with workspace filter for beta only
        let mut filters = SearchFilters::default();
        filters.workspaces.insert("/workspace/beta".into());

        let hits = client.search("needle", filters.clone(), 10, 0)?;

        // Property: all results must have workspace == "/workspace/beta"
        for hit in &hits {
            assert_eq!(
                hit.workspace, "/workspace/beta",
                "Workspace filter violated: got '{}' instead of '/workspace/beta'",
                hit.workspace
            );
        }
        assert!(!hits.is_empty(), "Should have found results");

        // Repeat search (should use cache)
        let cached_hits = client.search("needle", filters, 10, 0)?;
        for hit in &cached_hits {
            assert_eq!(
                hit.workspace, "/workspace/beta",
                "Cached search violated workspace filter"
            );
        }

        Ok(())
    }

    #[test]
    fn filter_fidelity_date_range_respected() -> Result<()> {
        // Multiple dates; filter should return only within range
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;

        // Early doc (ts=100)
        let conv_early = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("early".into()),
            workspace: None,
            source_path: dir.path().join("early.jsonl"),
            started_at: Some(100),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(100),
                content: "date range test".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        // Middle doc (ts=500)
        let conv_middle = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("middle".into()),
            workspace: None,
            source_path: dir.path().join("middle.jsonl"),
            started_at: Some(500),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(500),
                content: "date range test".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        // Late doc (ts=900)
        let conv_late = NormalizedConversation {
            agent_slug: "codex".into(),
            external_id: None,
            title: Some("late".into()),
            workspace: None,
            source_path: dir.path().join("late.jsonl"),
            started_at: Some(900),
            ended_at: None,
            metadata: serde_json::json!({}),
            messages: vec![NormalizedMessage {
                idx: 0,
                role: "user".into(),
                author: None,
                created_at: Some(900),
                content: "date range test".into(),
                extra: serde_json::json!({}),
                snippets: vec![],
            }],
        };
        index.add_conversation(&conv_early)?;
        index.add_conversation(&conv_middle)?;
        index.add_conversation(&conv_late)?;
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");

        // Filter for middle range only (400-600)
        let filters = SearchFilters {
            created_from: Some(400),
            created_to: Some(600),
            ..Default::default()
        };

        let hits = client.search("range", filters.clone(), 10, 0)?;

        // Property: all results must have created_at within [400, 600]
        for hit in &hits {
            if let Some(ts) = hit.created_at {
                assert!(
                    (400..=600).contains(&ts),
                    "Date range filter violated: got ts={} outside [400, 600]",
                    ts
                );
            }
        }
        // Should find only the middle doc
        assert_eq!(hits.len(), 1, "Should find exactly 1 doc in range");

        // Repeat search (cache)
        let cached_hits = client.search("range", filters, 10, 0)?;
        for hit in &cached_hits {
            if let Some(ts) = hit.created_at {
                assert!(
                    (400..=600).contains(&ts),
                    "Cached search violated date range filter"
                );
            }
        }

        Ok(())
    }

    #[test]
    fn filter_fidelity_combined_filters_respected() -> Result<()> {
        // Combine agent + workspace + date filters
        let dir = TempDir::new()?;
        let mut index = TantivyIndex::open_or_create(dir.path())?;

        // Create 4 docs with different combinations
        let combinations = [
            ("codex", "/ws/prod", 100),  // wrong date
            ("claude", "/ws/prod", 500), // correct agent, correct ws, correct date
            ("claude", "/ws/dev", 500),  // correct agent, wrong ws, correct date
            ("claude", "/ws/prod", 900), // correct agent, correct ws, wrong date
        ];

        for (i, (agent, ws, ts)) in combinations.iter().enumerate() {
            let conv = NormalizedConversation {
                agent_slug: (*agent).into(),
                external_id: None,
                title: Some(format!("combo-{i}")),
                workspace: Some(std::path::PathBuf::from(*ws)),
                source_path: dir.path().join(format!("{i}.jsonl")),
                started_at: Some(*ts),
                ended_at: None,
                metadata: serde_json::json!({}),
                messages: vec![NormalizedMessage {
                    idx: 0,
                    role: "user".into(),
                    author: None,
                    created_at: Some(*ts),
                    content: "hello world combotest query".into(),
                    extra: serde_json::json!({}),
                    snippets: vec![],
                }],
            };
            index.add_conversation(&conv)?;
        }
        index.commit()?;

        let client = SearchClient::open(dir.path(), None)?.expect("index present");

        // Filter: claude + /ws/prod + date 400-600
        let mut filters = SearchFilters::default();
        filters.agents.insert("claude".into());
        filters.workspaces.insert("/ws/prod".into());
        filters.created_from = Some(400);
        filters.created_to = Some(600);

        let hits = client.search("combotest", filters.clone(), 10, 0)?;

        // Should find exactly 1 doc (index 1 in combinations)
        assert_eq!(hits.len(), 1, "Combined filter should match exactly 1 doc");

        for hit in &hits {
            assert_eq!(hit.agent, "claude", "Agent filter violated");
            assert_eq!(hit.workspace, "/ws/prod", "Workspace filter violated");
            if let Some(ts) = hit.created_at {
                assert!((400..=600).contains(&ts), "Date filter violated: ts={ts}");
            }
        }

        // Cache hit
        let cached = client.search("combotest", filters, 10, 0)?;
        assert_eq!(cached.len(), 1, "Cached result count mismatch");

        Ok(())
    }

    #[test]
    fn filter_fidelity_cache_key_isolation() {
        // Different filters should have different cache keys
        let client = SearchClient {
            reader: None,
            sqlite: None,
            prefix_cache: Mutex::new(CacheShards::new(*CACHE_TOTAL_CAP, *CACHE_BYTE_CAP)),
            last_reload: Mutex::new(None),
            last_generation: Mutex::new(None),
            reload_epoch: Arc::new(AtomicU64::new(0)),
            warm_tx: None,
            _warm_handle: None,
            _shared_filters: Arc::new(Mutex::new(())),
            metrics: Metrics::default(),
            cache_namespace: format!("v{}|schema:test", CACHE_KEY_VERSION),
        };

        let filters_empty = SearchFilters::default();
        let mut filters_agent = SearchFilters::default();
        filters_agent.agents.insert("codex".into());

        let mut filters_ws = SearchFilters::default();
        filters_ws.workspaces.insert("/ws".into());

        let key_empty = client.cache_key("test", &filters_empty);
        let key_agent = client.cache_key("test", &filters_agent);
        let key_ws = client.cache_key("test", &filters_ws);

        // All keys should be different
        assert_ne!(
            key_empty, key_agent,
            "Empty vs agent filter keys should differ"
        );
        assert_ne!(
            key_empty, key_ws,
            "Empty vs workspace filter keys should differ"
        );
        assert_ne!(
            key_agent, key_ws,
            "Agent vs workspace filter keys should differ"
        );

        // Same filter should produce same key
        let mut filters_agent2 = SearchFilters::default();
        filters_agent2.agents.insert("codex".into());
        let key_agent2 = client.cache_key("test", &filters_agent2);
        assert_eq!(key_agent, key_agent2, "Same filter should produce same key");
    }
}
